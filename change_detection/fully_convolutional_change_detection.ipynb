{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "fully-convolutional-change-detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0fRdfk3E22X"
      },
      "source": [
        "# Fully Convolutional Networks for Change Detection\n",
        "\n",
        "Example code for training the network presented in the paper:\n",
        "\n",
        "```\n",
        "Daudt, R.C., Le Saux, B. and Boulch, A., 2018, October. Fully convolutional siamese networks for change detection. In 2018 25th IEEE International Conference on Image Processing (ICIP) (pp. 4063-4067). IEEE.\n",
        "```\n",
        "\n",
        "Code uses the OSCD dataset:\n",
        "\n",
        "```\n",
        "Daudt, R.C., Le Saux, B., Boulch, A. and Gousseau, Y., 2018, July. Urban change detection for multispectral earth observation using convolutional neural networks. In IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium (pp. 2115-2118). IEEE.\n",
        "```\n",
        "\n",
        "\n",
        "FresUNet architecture from paper:\n",
        "\n",
        "```\n",
        "Daudt, R.C., Le Saux, B., Boulch, A. and Gousseau, Y., 2019. Multitask learning for large-scale semantic change detection. Computer Vision and Image Understanding, 187, p.102783.\n",
        "```\n",
        "\n",
        "Please consider all relevant papers if you use this code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPxtbDGgE22h"
      },
      "source": [
        "# Rodrigo Daudt\n",
        "# rcdaudt.github.io\n",
        "# rodrigo.daudt@onera.fr"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZhSPUgaE22j",
        "outputId": "3526308a-aa6d-4096-95ca-eef39ad8e419"
      },
      "source": [
        "%%bash\n",
        "hostname"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f199cf76854f\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMmXyq4tE22j",
        "outputId": "1f1e9940-6cf4-453a-dc35-3b5500b17593"
      },
      "source": [
        "# Imports\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as tr\n",
        "\n",
        "# Models imported for .py files in local directory. Hashed out here - models just put in a cell.\n",
        "#from unet import Unet\n",
        "#from siamunet_conc import SiamUnet_conc\n",
        "#from siamunet_diff import SiamUnet_diff\n",
        "#from fresunet import FresUNet\n",
        "#from smallunet import SmallUnet\n",
        "#from smallunet_attempt import Unet\n",
        "\n",
        "# Other\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from skimage import io\n",
        "from scipy.ndimage import zoom\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm import tqdm as tqdm\n",
        "from pandas import read_csv\n",
        "from math import floor, ceil, sqrt, exp\n",
        "from IPython import display\n",
        "import time\n",
        "from itertools import chain\n",
        "import warnings\n",
        "from pprint import pprint\n",
        "\n",
        "print('IMPORTS OK')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IMPORTS OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feNT93S_FLmj",
        "outputId": "d028085a-9cc9-4235-993c-b03b6af010fa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMmB5KZlE22k",
        "outputId": "bcb1b038-2a54-4972-d54e-3c3b0debb235"
      },
      "source": [
        "# Global Variables' Definitions\n",
        "\n",
        "PATH_TO_DATASET = '/content/drive/MyDrive/onera/'\n",
        "IS_PROTOTYPE = False\n",
        "\n",
        "FP_MODIFIER = 1 # Tuning parameter, use 1 if unsure\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "PATCH_SIDE = 96\n",
        "N_EPOCHS = 50\n",
        "\n",
        "NORMALISE_IMGS = True\n",
        "\n",
        "TRAIN_STRIDE = int(PATCH_SIDE/2) - 1\n",
        "\n",
        "TYPE = 1 # 0-RGB | 1-RGBIr | 2-All bands s.t. resulution <= 20m | 3-All bands\n",
        "\n",
        "LOAD_TRAINED = False\n",
        "\n",
        "DATA_AUG = True\n",
        "\n",
        "\n",
        "print('DEFINITIONS OK')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEFINITIONS OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWJCCS-iE22k",
        "outputId": "7fa048f1-c112-4ae1-868a-cfb258d2f2c4"
      },
      "source": [
        "### This cell defines a load of functions that we will need to train the network e.g. data augmentation functions,\n",
        "### functions that call the different bands of the sentinel data, etc.\n",
        "\n",
        "# Functions\n",
        "\n",
        "def adjust_shape(I, s):\n",
        "    \"\"\"Adjust shape of grayscale image I to s.\"\"\"\n",
        "    \n",
        "    # crop if necesary\n",
        "    I = I[:s[0],:s[1]]\n",
        "    si = I.shape\n",
        "    \n",
        "    # pad if necessary \n",
        "    p0 = max(0,s[0] - si[0])\n",
        "    p1 = max(0,s[1] - si[1])\n",
        "    \n",
        "    return np.pad(I,((0,p0),(0,p1)),'edge')\n",
        "    \n",
        "\n",
        "def read_sentinel_img(path):\n",
        "    \"\"\"Read cropped Sentinel-2 image: RGB bands.\"\"\"\n",
        "    im_name = os.listdir(path)[0][:-7]\n",
        "    r = io.imread(path + im_name + \"B04.tif\")\n",
        "    g = io.imread(path + im_name + \"B03.tif\")\n",
        "    b = io.imread(path + im_name + \"B02.tif\")\n",
        "    \n",
        "    I = np.stack((r,g,b),axis=2).astype('float')\n",
        "    \n",
        "    if NORMALISE_IMGS:\n",
        "        I = (I - I.mean()) / I.std()\n",
        "\n",
        "    return I\n",
        "\n",
        "def read_sentinel_img_4(path):\n",
        "    \"\"\"Read cropped Sentinel-2 image: RGB and NIR bands.\"\"\"\n",
        "    im_name = os.listdir(path)[0][:-7]\n",
        "    r = io.imread(path + im_name + \"B04.tif\")\n",
        "    g = io.imread(path + im_name + \"B03.tif\")\n",
        "    b = io.imread(path + im_name + \"B02.tif\")\n",
        "    nir = io.imread(path + im_name + \"B08.tif\")\n",
        "    \n",
        "    I = np.stack((r,g,b,nir),axis=2).astype('float')\n",
        "    \n",
        "    if NORMALISE_IMGS:\n",
        "        I = (I - I.mean()) / I.std()\n",
        "\n",
        "    return I\n",
        "\n",
        "def read_sentinel_img_leq20(path):\n",
        "    \"\"\"Read cropped Sentinel-2 image: bands with resolution less than or equals to 20m.\"\"\"\n",
        "    im_name = os.listdir(path)[0][:-7]\n",
        "    \n",
        "    r = io.imread(path + im_name + \"B04.tif\")\n",
        "    s = r.shape\n",
        "    g = io.imread(path + im_name + \"B03.tif\")\n",
        "    b = io.imread(path + im_name + \"B02.tif\")\n",
        "    nir = io.imread(path + im_name + \"B08.tif\")\n",
        "    \n",
        "    ir1 = adjust_shape(zoom(io.imread(path + im_name + \"B05.tif\"),2),s)\n",
        "    ir2 = adjust_shape(zoom(io.imread(path + im_name + \"B06.tif\"),2),s)\n",
        "    ir3 = adjust_shape(zoom(io.imread(path + im_name + \"B07.tif\"),2),s)\n",
        "    nir2 = adjust_shape(zoom(io.imread(path + im_name + \"B8A.tif\"),2),s)\n",
        "    swir2 = adjust_shape(zoom(io.imread(path + im_name + \"B11.tif\"),2),s)\n",
        "    swir3 = adjust_shape(zoom(io.imread(path + im_name + \"B12.tif\"),2),s)\n",
        "    \n",
        "    I = np.stack((r,g,b,nir,ir1,ir2,ir3,nir2,swir2,swir3),axis=2).astype('float')\n",
        "    \n",
        "    if NORMALISE_IMGS:\n",
        "        I = (I - I.mean()) / I.std()\n",
        "\n",
        "    return I\n",
        "\n",
        "def read_sentinel_img_leq60(path):\n",
        "    \"\"\"Read cropped Sentinel-2 image: all bands.\"\"\"\n",
        "    im_name = os.listdir(path)[0][:-7]\n",
        "    \n",
        "    r = io.imread(path + im_name + \"B04.tif\")\n",
        "    s = r.shape\n",
        "    g = io.imread(path + im_name + \"B03.tif\")\n",
        "    b = io.imread(path + im_name + \"B02.tif\")\n",
        "    nir = io.imread(path + im_name + \"B08.tif\")\n",
        "    \n",
        "    ir1 = adjust_shape(zoom(io.imread(path + im_name + \"B05.tif\"),2),s)\n",
        "    ir2 = adjust_shape(zoom(io.imread(path + im_name + \"B06.tif\"),2),s)\n",
        "    ir3 = adjust_shape(zoom(io.imread(path + im_name + \"B07.tif\"),2),s)\n",
        "    nir2 = adjust_shape(zoom(io.imread(path + im_name + \"B8A.tif\"),2),s)\n",
        "    swir2 = adjust_shape(zoom(io.imread(path + im_name + \"B11.tif\"),2),s)\n",
        "    swir3 = adjust_shape(zoom(io.imread(path + im_name + \"B12.tif\"),2),s)\n",
        "    \n",
        "    uv = adjust_shape(zoom(io.imread(path + im_name + \"B01.tif\"),6),s)\n",
        "    wv = adjust_shape(zoom(io.imread(path + im_name + \"B09.tif\"),6),s)\n",
        "    swirc = adjust_shape(zoom(io.imread(path + im_name + \"B10.tif\"),6),s)\n",
        "    \n",
        "    I = np.stack((r,g,b,nir,ir1,ir2,ir3,nir2,swir2,swir3,uv,wv,swirc),axis=2).astype('float')\n",
        "    \n",
        "    if NORMALISE_IMGS:\n",
        "        I = (I - I.mean()) / I.std()\n",
        "\n",
        "    return I\n",
        "\n",
        "def read_sentinel_img_trio(path):\n",
        "    \"\"\"Read cropped Sentinel-2 image pair and change map.\"\"\"\n",
        "#     read images\n",
        "    if TYPE == 0:\n",
        "        I1 = read_sentinel_img(path + '/imgs_1/')\n",
        "        I2 = read_sentinel_img(path + '/imgs_2/')\n",
        "    elif TYPE == 1:\n",
        "        I1 = read_sentinel_img_4(path + '/imgs_1/')\n",
        "        I2 = read_sentinel_img_4(path + '/imgs_2/')\n",
        "    elif TYPE == 2:\n",
        "        I1 = read_sentinel_img_leq20(path + '/imgs_1/')\n",
        "        I2 = read_sentinel_img_leq20(path + '/imgs_2/')\n",
        "    elif TYPE == 3:\n",
        "        I1 = read_sentinel_img_leq60(path + '/imgs_1/')\n",
        "        I2 = read_sentinel_img_leq60(path + '/imgs_2/')\n",
        "        \n",
        "    cm = io.imread(path + '/cm/cm.png', as_gray=True) != 0\n",
        "    \n",
        "    # crop if necessary\n",
        "    s1 = I1.shape\n",
        "    s2 = I2.shape\n",
        "    I2 = np.pad(I2,((0, s1[0] - s2[0]), (0, s1[1] - s2[1]), (0,0)),'edge')\n",
        "    \n",
        "    \n",
        "    return I1, I2, cm\n",
        "\n",
        "\n",
        "\n",
        "def reshape_for_torch(I):\n",
        "    \"\"\"Transpose image for PyTorch coordinates.\"\"\"\n",
        "#     out = np.swapaxes(I,1,2)\n",
        "#     out = np.swapaxes(out,0,1)\n",
        "#     out = out[np.newaxis,:]\n",
        "    out = I.transpose((2, 0, 1))\n",
        "    return torch.from_numpy(out)\n",
        "\n",
        "\n",
        "\n",
        "class ChangeDetectionDataset(Dataset):\n",
        "    \"\"\"Change Detection dataset class, used for both training and test data.\"\"\"\n",
        "\n",
        "    def __init__(self, path, train = True, patch_side = 96, stride = None, use_all_bands = False, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        \n",
        "        # basics\n",
        "        self.transform = transform\n",
        "        self.path = path\n",
        "        self.patch_side = patch_side\n",
        "        if not stride:\n",
        "            self.stride = 1\n",
        "        else:\n",
        "            self.stride = stride\n",
        "        \n",
        "        if train:\n",
        "            fname = 'train.txt'\n",
        "        else:\n",
        "            fname = 'test.txt'\n",
        "        \n",
        "#         print(path + fname)\n",
        "        self.names = read_csv(path + fname).columns\n",
        "        self.n_imgs = self.names.shape[0]\n",
        "        \n",
        "        n_pix = 0\n",
        "        true_pix = 0\n",
        "        \n",
        "        \n",
        "        # load images\n",
        "        self.imgs_1 = {}\n",
        "        self.imgs_2 = {}\n",
        "        self.change_maps = {}\n",
        "        self.n_patches_per_image = {}\n",
        "        self.n_patches = 0\n",
        "        self.patch_coords = []\n",
        "        for im_name in tqdm(self.names):\n",
        "            # load and store each image\n",
        "            I1, I2, cm = read_sentinel_img_trio(self.path + im_name)\n",
        "            self.imgs_1[im_name] = reshape_for_torch(I1)\n",
        "            self.imgs_2[im_name] = reshape_for_torch(I2)\n",
        "            self.change_maps[im_name] = cm\n",
        "            \n",
        "            s = cm.shape\n",
        "            n_pix += np.prod(s)\n",
        "            true_pix += cm.sum()\n",
        "            \n",
        "            # calculate the number of patches\n",
        "            s = self.imgs_1[im_name].shape\n",
        "            n1 = ceil((s[1] - self.patch_side + 1) / self.stride)\n",
        "            n2 = ceil((s[2] - self.patch_side + 1) / self.stride)\n",
        "            n_patches_i = n1 * n2\n",
        "            self.n_patches_per_image[im_name] = n_patches_i\n",
        "            self.n_patches += n_patches_i\n",
        "            \n",
        "            # generate path coordinates\n",
        "            for i in range(n1):\n",
        "                for j in range(n2):\n",
        "                    # coordinates in (x1, x2, y1, y2)\n",
        "                    current_patch_coords = (im_name, \n",
        "                                    [self.stride*i, self.stride*i + self.patch_side, self.stride*j, self.stride*j + self.patch_side],\n",
        "                                    [self.stride*(i + 1), self.stride*(j + 1)])\n",
        "                    self.patch_coords.append(current_patch_coords)\n",
        "                    \n",
        "        self.weights = [ FP_MODIFIER * 2 * true_pix / n_pix, 2 * (n_pix - true_pix) / n_pix]\n",
        "        \n",
        "        \n",
        "\n",
        "    def get_img(self, im_name):\n",
        "        return self.imgs_1[im_name], self.imgs_2[im_name], self.change_maps[im_name]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_patches\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        current_patch_coords = self.patch_coords[idx]\n",
        "        im_name = current_patch_coords[0]\n",
        "        limits = current_patch_coords[1]\n",
        "        centre = current_patch_coords[2]\n",
        "        \n",
        "        I1 = self.imgs_1[im_name][:, limits[0]:limits[1], limits[2]:limits[3]]\n",
        "        I2 = self.imgs_2[im_name][:, limits[0]:limits[1], limits[2]:limits[3]]\n",
        "        \n",
        "        label = self.change_maps[im_name][limits[0]:limits[1], limits[2]:limits[3]]\n",
        "        label = torch.from_numpy(1*np.array(label)).float()\n",
        "        \n",
        "        sample = {'I1': I1, 'I2': I2, 'label': label}\n",
        "        \n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "\n",
        "class RandomFlip(object):\n",
        "    \"\"\"Flip randomly the images in a sample.\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         return\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        I1, I2, label = sample['I1'], sample['I2'], sample['label']\n",
        "        \n",
        "        if random.random() > 0.5:\n",
        "            I1 =  I1.numpy()[:,:,::-1].copy()\n",
        "            I1 = torch.from_numpy(I1)\n",
        "            I2 =  I2.numpy()[:,:,::-1].copy()\n",
        "            I2 = torch.from_numpy(I2)\n",
        "            label =  label.numpy()[:,::-1].copy()\n",
        "            label = torch.from_numpy(label)\n",
        "\n",
        "        return {'I1': I1, 'I2': I2, 'label': label}\n",
        "\n",
        "\n",
        "\n",
        "class RandomRot(object):\n",
        "    \"\"\"Rotate randomly the images in a sample.\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         return\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        I1, I2, label = sample['I1'], sample['I2'], sample['label']\n",
        "        \n",
        "        n = random.randint(0, 3)\n",
        "        if n:\n",
        "            I1 =  sample['I1'].numpy()\n",
        "            I1 = np.rot90(I1, n, axes=(1, 2)).copy()\n",
        "            I1 = torch.from_numpy(I1)\n",
        "            I2 =  sample['I2'].numpy()\n",
        "            I2 = np.rot90(I2, n, axes=(1, 2)).copy()\n",
        "            I2 = torch.from_numpy(I2)\n",
        "            label =  sample['label'].numpy()\n",
        "            label = np.rot90(label, n, axes=(0, 1)).copy()\n",
        "            label = torch.from_numpy(label)\n",
        "\n",
        "        return {'I1': I1, 'I2': I2, 'label': label}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('UTILS OK')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UTILS OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWB5lipxWFhS"
      },
      "source": [
        "# Daudt, R.C., Le Saux, B., Boulch, A. and Gousseau, Y., 2019. Multitask learning for large-scale semantic change detection. Computer Vision and Image Understanding, 187, p.102783.\r\n",
        "\r\n",
        "# FresUNet - comes from the above paper. Still not sure how it improves on UNet tbh. Will find out soon.\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.nn.modules.padding import ReplicationPad2d\r\n",
        "\r\n",
        "def conv3x3(in_planes, out_planes, stride=1):\r\n",
        "    \"3x3 convolution with padding\"\r\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1)\r\n",
        "\r\n",
        "\r\n",
        "class BasicBlock_ss(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, inplanes, planes = None, subsamp=1):\r\n",
        "        super(BasicBlock_ss, self).__init__()\r\n",
        "        if planes == None:\r\n",
        "            planes = inplanes * subsamp\r\n",
        "        self.conv1 = conv3x3(inplanes, planes)\r\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "        self.conv2 = conv3x3(planes, planes)\r\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\r\n",
        "        self.subsamp = subsamp\r\n",
        "        self.doit = planes != inplanes\r\n",
        "        if self.doit:\r\n",
        "            self.couple = nn.Conv2d(inplanes, planes, kernel_size=1)\r\n",
        "            self.bnc = nn.BatchNorm2d(planes)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        if self.doit:\r\n",
        "            residual = self.couple(x)\r\n",
        "            residual = self.bnc(residual)\r\n",
        "        else:\r\n",
        "            residual = x\r\n",
        "\r\n",
        "        out = self.conv1(x)\r\n",
        "        out = self.bn1(out)\r\n",
        "        out = self.relu(out)\r\n",
        "        \r\n",
        "        if self.subsamp > 1:\r\n",
        "            out = F.max_pool2d(out, kernel_size=self.subsamp, stride=self.subsamp)\r\n",
        "            residual = F.max_pool2d(residual, kernel_size=self.subsamp, stride=self.subsamp)\r\n",
        "\r\n",
        "        out = self.conv2(out)\r\n",
        "        out = self.bn2(out)\r\n",
        "        \r\n",
        "        out += residual\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        return out\r\n",
        "    \r\n",
        "\r\n",
        "    \r\n",
        "class BasicBlock_us(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, inplanes, upsamp=1):\r\n",
        "        super(BasicBlock_us, self).__init__()\r\n",
        "        planes = int(inplanes / upsamp) # assumes integer result, fix later\r\n",
        "        self.conv1 = nn.ConvTranspose2d(inplanes, planes, kernel_size=3, padding=1, stride=upsamp, output_padding=1)\r\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "        self.conv2 = conv3x3(planes, planes)\r\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\r\n",
        "        self.upsamp = upsamp\r\n",
        "        self.couple = nn.ConvTranspose2d(inplanes, planes, kernel_size=3, padding=1, stride=upsamp, output_padding=1) \r\n",
        "        self.bnc = nn.BatchNorm2d(planes)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        residual = self.couple(x)\r\n",
        "        residual = self.bnc(residual)\r\n",
        "\r\n",
        "        out = self.conv1(x)\r\n",
        "        out = self.bn1(out)\r\n",
        "        out = self.relu(out)\r\n",
        "        \r\n",
        "\r\n",
        "        out = self.conv2(out)\r\n",
        "        out = self.bn2(out)\r\n",
        "\r\n",
        "        out += residual\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        return out\r\n",
        "    \r\n",
        "    \r\n",
        "class FresUNet(nn.Module):\r\n",
        "    \"\"\"FresUNet segmentation network.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, input_nbr, label_nbr):\r\n",
        "        \"\"\"Init FresUNet fields.\"\"\"\r\n",
        "        super(FresUNet, self).__init__()\r\n",
        "\r\n",
        "        self.input_nbr = input_nbr\r\n",
        "        \r\n",
        "        cur_depth = input_nbr\r\n",
        "        \r\n",
        "        base_depth = 8\r\n",
        "        \r\n",
        "        # Encoding stage 1\r\n",
        "        self.encres1_1 = BasicBlock_ss(cur_depth, planes = base_depth)\r\n",
        "        cur_depth = base_depth\r\n",
        "        d1 = base_depth\r\n",
        "        self.encres1_2 = BasicBlock_ss(cur_depth, subsamp=2)\r\n",
        "        cur_depth *= 2\r\n",
        "        \r\n",
        "        # Encoding stage 2\r\n",
        "        self.encres2_1 = BasicBlock_ss(cur_depth)\r\n",
        "        d2 = cur_depth\r\n",
        "        self.encres2_2 = BasicBlock_ss(cur_depth, subsamp=2)\r\n",
        "        cur_depth *= 2\r\n",
        "        \r\n",
        "        # Encoding stage 3\r\n",
        "        self.encres3_1 = BasicBlock_ss(cur_depth)\r\n",
        "        d3 = cur_depth\r\n",
        "        self.encres3_2 = BasicBlock_ss(cur_depth, subsamp=2)\r\n",
        "        cur_depth *= 2\r\n",
        "        \r\n",
        "        # Encoding stage 4\r\n",
        "        self.encres4_1 = BasicBlock_ss(cur_depth)\r\n",
        "        d4 = cur_depth\r\n",
        "        self.encres4_2 = BasicBlock_ss(cur_depth, subsamp=2)\r\n",
        "        cur_depth *= 2\r\n",
        "        \r\n",
        "        # Decoding stage 4\r\n",
        "        self.decres4_1 = BasicBlock_ss(cur_depth)\r\n",
        "        self.decres4_2 = BasicBlock_us(cur_depth, upsamp=2)\r\n",
        "        cur_depth = int(cur_depth/2)\r\n",
        "        \r\n",
        "        # Decoding stage 3\r\n",
        "        self.decres3_1 = BasicBlock_ss(cur_depth + d4, planes = cur_depth)\r\n",
        "        self.decres3_2 = BasicBlock_us(cur_depth, upsamp=2)\r\n",
        "        cur_depth = int(cur_depth/2)\r\n",
        "        \r\n",
        "        # Decoding stage 2\r\n",
        "        self.decres2_1 = BasicBlock_ss(cur_depth + d3, planes = cur_depth)\r\n",
        "        self.decres2_2 = BasicBlock_us(cur_depth, upsamp=2)\r\n",
        "        cur_depth = int(cur_depth/2)\r\n",
        "        \r\n",
        "        # Decoding stage 1\r\n",
        "        self.decres1_1 = BasicBlock_ss(cur_depth + d2, planes = cur_depth)\r\n",
        "        self.decres1_2 = BasicBlock_us(cur_depth, upsamp=2)\r\n",
        "        cur_depth = int(cur_depth/2)\r\n",
        "        \r\n",
        "        # Output\r\n",
        "        self.coupling = nn.Conv2d(cur_depth + d1, label_nbr, kernel_size=1)\r\n",
        "        self.sm = nn.LogSoftmax(dim=1)\r\n",
        "        \r\n",
        "    def forward(self, x1, x2):\r\n",
        "\r\n",
        "        x = torch.cat((x1, x2), 1)\r\n",
        "        \r\n",
        "#         pad5 = ReplicationPad2d((0, x53.size(3) - x5d.size(3), 0, x53.size(2) - x5d.size(2)))\r\n",
        "        \r\n",
        "        s1_1 = x.size()\r\n",
        "        x1 = self.encres1_1(x)\r\n",
        "        x = self.encres1_2(x1)\r\n",
        "        \r\n",
        "        s2_1 = x.size()\r\n",
        "        x2 = self.encres2_1(x)\r\n",
        "        x = self.encres2_2(x2)\r\n",
        "        \r\n",
        "        s3_1 = x.size()\r\n",
        "        x3 = self.encres3_1(x)\r\n",
        "        x = self.encres3_2(x3)\r\n",
        "        \r\n",
        "        s4_1 = x.size()\r\n",
        "        x4 = self.encres4_1(x)\r\n",
        "        x = self.encres4_2(x4)\r\n",
        "        \r\n",
        "        x = self.decres4_1(x)\r\n",
        "        x = self.decres4_2(x)\r\n",
        "        s4_2 = x.size()\r\n",
        "        pad4 = ReplicationPad2d((0, s4_1[3] - s4_2[3], 0, s4_1[2] - s4_2[2]))\r\n",
        "        x = pad4(x)\r\n",
        "        \r\n",
        "        # x = self.decres3_1(x)\r\n",
        "        x = self.decres3_1(torch.cat((x, x4), 1))\r\n",
        "        x = self.decres3_2(x)\r\n",
        "        s3_2 = x.size()\r\n",
        "        pad3 = ReplicationPad2d((0, s3_1[3] - s3_2[3], 0, s3_1[2] - s3_2[2]))\r\n",
        "        x = pad3(x)\r\n",
        "        \r\n",
        "        x = self.decres2_1(torch.cat((x, x3), 1))\r\n",
        "        x = self.decres2_2(x)\r\n",
        "        s2_2 = x.size()\r\n",
        "        pad2 = ReplicationPad2d((0, s2_1[3] - s2_2[3], 0, s2_1[2] - s2_2[2]))\r\n",
        "        x = pad2(x)\r\n",
        "        \r\n",
        "        x = self.decres1_1(torch.cat((x, x2), 1))\r\n",
        "        x = self.decres1_2(x)\r\n",
        "        s1_2 = x.size()\r\n",
        "        pad1 = ReplicationPad2d((0, s1_1[3] - s1_2[3], 0, s1_1[2] - s1_2[2]))\r\n",
        "        x = pad1(x)\r\n",
        "        \r\n",
        "        x = self.coupling(torch.cat((x, x1), 1))\r\n",
        "        x = self.sm(x)\r\n",
        "        \r\n",
        "        return x"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOOai5r9E22s"
      },
      "source": [
        "### Simple UNet implementation\r\n",
        "\r\n",
        "#import torch\r\n",
        "#import torch.nn as nn\r\n",
        "#import torch.nn.functional as F\r\n",
        "#from torch.nn.modules.padding import ReplicationPad2d\r\n",
        "\r\n",
        "class Unet(nn.Module):\r\n",
        "    \"\"\"EF segmentation network.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, input_nbr, label_nbr):\r\n",
        "        super(Unet, self).__init__()\r\n",
        "\r\n",
        "        self.input_nbr = input_nbr\r\n",
        "\r\n",
        "        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\r\n",
        "        self.bn11 = nn.BatchNorm2d(16)\r\n",
        "        self.do11 = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\r\n",
        "        self.bn12 = nn.BatchNorm2d(16)\r\n",
        "        self.do12 = nn.Dropout2d(p=0.2)\r\n",
        "\r\n",
        "        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\r\n",
        "        self.bn21 = nn.BatchNorm2d(32)\r\n",
        "        self.do21 = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\r\n",
        "        self.bn22 = nn.BatchNorm2d(32)\r\n",
        "        self.do22 = nn.Dropout2d(p=0.2)\r\n",
        "\r\n",
        "        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\r\n",
        "        self.bn31 = nn.BatchNorm2d(64)\r\n",
        "        self.do31 = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\r\n",
        "        self.bn32 = nn.BatchNorm2d(64)\r\n",
        "        self.do32 = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\r\n",
        "        self.bn33 = nn.BatchNorm2d(64)\r\n",
        "        self.do33 = nn.Dropout2d(p=0.2)\r\n",
        "\r\n",
        "        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\r\n",
        "        self.bn41 = nn.BatchNorm2d(128)\r\n",
        "        self.do41 = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\r\n",
        "        self.bn42 = nn.BatchNorm2d(128)\r\n",
        "        self.do42 = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\r\n",
        "        self.bn43 = nn.BatchNorm2d(128)\r\n",
        "        self.do43 = nn.Dropout2d(p=0.2)\r\n",
        "\r\n",
        "\r\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\r\n",
        "\r\n",
        "        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\r\n",
        "        self.bn43d = nn.BatchNorm2d(128)\r\n",
        "        self.do43d = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\r\n",
        "        self.bn42d = nn.BatchNorm2d(128)\r\n",
        "        self.do42d = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\r\n",
        "        self.bn41d = nn.BatchNorm2d(64)\r\n",
        "        self.do41d = nn.Dropout2d(p=0.2)\r\n",
        "\r\n",
        "        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\r\n",
        "\r\n",
        "        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\r\n",
        "        self.bn33d = nn.BatchNorm2d(64)\r\n",
        "        self.do33d = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\r\n",
        "        self.bn32d = nn.BatchNorm2d(64)\r\n",
        "        self.do32d = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\r\n",
        "        self.bn31d = nn.BatchNorm2d(32)\r\n",
        "        self.do31d = nn.Dropout2d(p=0.2)\r\n",
        "\r\n",
        "        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\r\n",
        "\r\n",
        "        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\r\n",
        "        self.bn22d = nn.BatchNorm2d(32)\r\n",
        "        self.do22d = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\r\n",
        "        self.bn21d = nn.BatchNorm2d(16)\r\n",
        "        self.do21d = nn.Dropout2d(p=0.2)\r\n",
        "\r\n",
        "        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\r\n",
        "\r\n",
        "        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\r\n",
        "        self.bn12d = nn.BatchNorm2d(16)\r\n",
        "        self.do12d = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\r\n",
        "\r\n",
        "        self.sm = nn.LogSoftmax(dim=1)\r\n",
        "\r\n",
        "    def forward(self, x1, x2):\r\n",
        "\r\n",
        "        x = torch.cat((x1, x2), 1)\r\n",
        "\r\n",
        "        \"\"\"Forward method.\"\"\"\r\n",
        "        # Stage 1\r\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x))))\r\n",
        "        x12 = self.do12(F.relu(self.bn12(self.conv12(x11))))\r\n",
        "        x1p = F.max_pool2d(x12, kernel_size=2, stride=2)\r\n",
        "\r\n",
        "        # Stage 2\r\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\r\n",
        "        x22 = self.do22(F.relu(self.bn22(self.conv22(x21))))\r\n",
        "        x2p = F.max_pool2d(x22, kernel_size=2, stride=2)\r\n",
        "\r\n",
        "        # Stage 3\r\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\r\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\r\n",
        "        x33 = self.do33(F.relu(self.bn33(self.conv33(x32))))\r\n",
        "        x3p = F.max_pool2d(x33, kernel_size=2, stride=2)\r\n",
        "\r\n",
        "        # Stage 4\r\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\r\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\r\n",
        "        x43 = self.do43(F.relu(self.bn43(self.conv43(x42))))\r\n",
        "        x4p = F.max_pool2d(x43, kernel_size=2, stride=2)\r\n",
        "\r\n",
        "\r\n",
        "        # Stage 4d\r\n",
        "        x4d = self.upconv4(x4p)\r\n",
        "        pad4 = ReplicationPad2d((0, x43.size(3) - x4d.size(3), 0, x43.size(2) - x4d.size(2)))\r\n",
        "        x4d = torch.cat((pad4(x4d), x43), 1)\r\n",
        "        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\r\n",
        "        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\r\n",
        "        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\r\n",
        "\r\n",
        "        # Stage 3d\r\n",
        "        x3d = self.upconv3(x41d)\r\n",
        "        pad3 = ReplicationPad2d((0, x33.size(3) - x3d.size(3), 0, x33.size(2) - x3d.size(2)))\r\n",
        "        x3d = torch.cat((pad3(x3d), x33), 1)\r\n",
        "        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\r\n",
        "        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\r\n",
        "        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\r\n",
        "\r\n",
        "        # Stage 2d\r\n",
        "        x2d = self.upconv2(x31d)\r\n",
        "        pad2 = ReplicationPad2d((0, x22.size(3) - x2d.size(3), 0, x22.size(2) - x2d.size(2)))\r\n",
        "        x2d = torch.cat((pad2(x2d), x22), 1)\r\n",
        "        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\r\n",
        "        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\r\n",
        "\r\n",
        "        # Stage 1d\r\n",
        "        x1d = self.upconv1(x21d)\r\n",
        "        pad1 = ReplicationPad2d((0, x12.size(3) - x1d.size(3), 0, x12.size(2) - x1d.size(2)))\r\n",
        "        x1d = torch.cat((pad1(x1d), x12), 1)\r\n",
        "        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\r\n",
        "        x11d = self.conv11d(x12d)\r\n",
        "\r\n",
        "        return self.sm(x11d)\r\n",
        "\r\n",
        "    \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7PjfdRoXPIS"
      },
      "source": [
        "# Daudt, R. C., Le Saux, B., & Boulch, A. \"Fully convolutional siamese networks for change detection\". In 2018 25th IEEE International Conference on Image Processing (ICIP) (pp. 4063-4067). IEEE.\r\n",
        "\r\n",
        "### SiamUNet_conc network. Improvement on simple UNet, as outlined in the paper above. Siamese architectures are pretty nifty.\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.nn.modules.padding import ReplicationPad2d\r\n",
        "\r\n",
        "class SiamUnet_conc(nn.Module):\r\n",
        "    \"\"\"SiamUnet_conc segmentation network.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, input_nbr, label_nbr):\r\n",
        "        super(SiamUnet_conc, self).__init__()\r\n",
        "\r\n",
        "        self.input_nbr = input_nbr\r\n",
        "\r\n",
        "        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\r\n",
        "        self.bn11 = nn.BatchNorm2d(16)\r\n",
        "        self.do11 = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\r\n",
        "        self.bn12 = nn.BatchNorm2d(16)\r\n",
        "        self.do12 = nn.Dropout2d(p=0.2)\r\n",
        "\r\n",
        "        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\r\n",
        "        self.bn21 = nn.BatchNorm2d(32)\r\n",
        "        self.do21 = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\r\n",
        "        self.bn22 = nn.BatchNorm2d(32)\r\n",
        "        self.do22 = nn.Dropout2d(p=0.2)\r\n",
        "\r\n",
        "        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\r\n",
        "        self.bn31 = nn.BatchNorm2d(64)\r\n",
        "        self.do31 = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\r\n",
        "        self.bn32 = nn.BatchNorm2d(64)\r\n",
        "        self.do32 = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\r\n",
        "        self.bn33 = nn.BatchNorm2d(64)\r\n",
        "        self.do33 = nn.Dropout2d(p=0.2)\r\n",
        "\r\n",
        "        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\r\n",
        "        self.bn41 = nn.BatchNorm2d(128)\r\n",
        "        self.do41 = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\r\n",
        "        self.bn42 = nn.BatchNorm2d(128)\r\n",
        "        self.do42 = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\r\n",
        "        self.bn43 = nn.BatchNorm2d(128)\r\n",
        "        self.do43 = nn.Dropout2d(p=0.2)\r\n",
        "\r\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\r\n",
        "\r\n",
        "        self.conv43d = nn.ConvTranspose2d(384, 128, kernel_size=3, padding=1)\r\n",
        "        self.bn43d = nn.BatchNorm2d(128)\r\n",
        "        self.do43d = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\r\n",
        "        self.bn42d = nn.BatchNorm2d(128)\r\n",
        "        self.do42d = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\r\n",
        "        self.bn41d = nn.BatchNorm2d(64)\r\n",
        "        self.do41d = nn.Dropout2d(p=0.2)\r\n",
        "\r\n",
        "        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\r\n",
        "\r\n",
        "        self.conv33d = nn.ConvTranspose2d(192, 64, kernel_size=3, padding=1)\r\n",
        "        self.bn33d = nn.BatchNorm2d(64)\r\n",
        "        self.do33d = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\r\n",
        "        self.bn32d = nn.BatchNorm2d(64)\r\n",
        "        self.do32d = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\r\n",
        "        self.bn31d = nn.BatchNorm2d(32)\r\n",
        "        self.do31d = nn.Dropout2d(p=0.2)\r\n",
        "\r\n",
        "        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\r\n",
        "\r\n",
        "        self.conv22d = nn.ConvTranspose2d(96, 32, kernel_size=3, padding=1)\r\n",
        "        self.bn22d = nn.BatchNorm2d(32)\r\n",
        "        self.do22d = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\r\n",
        "        self.bn21d = nn.BatchNorm2d(16)\r\n",
        "        self.do21d = nn.Dropout2d(p=0.2)\r\n",
        "\r\n",
        "        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\r\n",
        "\r\n",
        "        self.conv12d = nn.ConvTranspose2d(48, 16, kernel_size=3, padding=1)\r\n",
        "        self.bn12d = nn.BatchNorm2d(16)\r\n",
        "        self.do12d = nn.Dropout2d(p=0.2)\r\n",
        "        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\r\n",
        "\r\n",
        "        self.sm = nn.LogSoftmax(dim=1)\r\n",
        "\r\n",
        "    def forward(self, x1, x2):\r\n",
        "\r\n",
        "        \"\"\"Forward method.\"\"\"\r\n",
        "        # Stage 1\r\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x1))))\r\n",
        "        x12_1 = self.do12(F.relu(self.bn12(self.conv12(x11))))\r\n",
        "        x1p = F.max_pool2d(x12_1, kernel_size=2, stride=2)\r\n",
        "\r\n",
        "\r\n",
        "        # Stage 2\r\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\r\n",
        "        x22_1 = self.do22(F.relu(self.bn22(self.conv22(x21))))\r\n",
        "        x2p = F.max_pool2d(x22_1, kernel_size=2, stride=2)\r\n",
        "\r\n",
        "        # Stage 3\r\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\r\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\r\n",
        "        x33_1 = self.do33(F.relu(self.bn33(self.conv33(x32))))\r\n",
        "        x3p = F.max_pool2d(x33_1, kernel_size=2, stride=2)\r\n",
        "\r\n",
        "        # Stage 4\r\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\r\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\r\n",
        "        x43_1 = self.do43(F.relu(self.bn43(self.conv43(x42))))\r\n",
        "        x4p = F.max_pool2d(x43_1, kernel_size=2, stride=2)\r\n",
        "\r\n",
        "\r\n",
        "        ####################################################\r\n",
        "        # Stage 1\r\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x2))))\r\n",
        "        x12_2 = self.do12(F.relu(self.bn12(self.conv12(x11))))\r\n",
        "        x1p = F.max_pool2d(x12_2, kernel_size=2, stride=2)\r\n",
        "\r\n",
        "        # Stage 2\r\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\r\n",
        "        x22_2 = self.do22(F.relu(self.bn22(self.conv22(x21))))\r\n",
        "        x2p = F.max_pool2d(x22_2, kernel_size=2, stride=2)\r\n",
        "\r\n",
        "        # Stage 3\r\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\r\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\r\n",
        "        x33_2 = self.do33(F.relu(self.bn33(self.conv33(x32))))\r\n",
        "        x3p = F.max_pool2d(x33_2, kernel_size=2, stride=2)\r\n",
        "\r\n",
        "        # Stage 4\r\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\r\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\r\n",
        "        x43_2 = self.do43(F.relu(self.bn43(self.conv43(x42))))\r\n",
        "        x4p = F.max_pool2d(x43_2, kernel_size=2, stride=2)\r\n",
        "\r\n",
        "\r\n",
        "        ####################################################\r\n",
        "        # Stage 4d\r\n",
        "        x4d = self.upconv4(x4p)\r\n",
        "        pad4 = ReplicationPad2d((0, x43_1.size(3) - x4d.size(3), 0, x43_1.size(2) - x4d.size(2)))\r\n",
        "        x4d = torch.cat((pad4(x4d), x43_1, x43_2), 1)\r\n",
        "        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\r\n",
        "        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\r\n",
        "        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\r\n",
        "\r\n",
        "        # Stage 3d\r\n",
        "        x3d = self.upconv3(x41d)\r\n",
        "        pad3 = ReplicationPad2d((0, x33_1.size(3) - x3d.size(3), 0, x33_1.size(2) - x3d.size(2)))\r\n",
        "        x3d = torch.cat((pad3(x3d), x33_1, x33_2), 1)\r\n",
        "        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\r\n",
        "        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\r\n",
        "        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\r\n",
        "\r\n",
        "        # Stage 2d\r\n",
        "        x2d = self.upconv2(x31d)\r\n",
        "        pad2 = ReplicationPad2d((0, x22_1.size(3) - x2d.size(3), 0, x22_1.size(2) - x2d.size(2)))\r\n",
        "        x2d = torch.cat((pad2(x2d), x22_1, x22_2), 1)\r\n",
        "        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\r\n",
        "        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\r\n",
        "\r\n",
        "        # Stage 1d\r\n",
        "        x1d = self.upconv1(x21d)\r\n",
        "        pad1 = ReplicationPad2d((0, x12_1.size(3) - x1d.size(3), 0, x12_1.size(2) - x1d.size(2)))\r\n",
        "        x1d = torch.cat((pad1(x1d), x12_1, x12_2), 1)\r\n",
        "        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\r\n",
        "        x11d = self.conv11d(x12d)\r\n",
        "\r\n",
        "        return self.sm(x11d)\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC_LwYkiE22x",
        "outputId": "0689cb82-3225-4cd9-bf06-287c74b18141"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "\n",
        "if DATA_AUG:\n",
        "    data_transform = tr.Compose([RandomFlip(), RandomRot()])\n",
        "else:\n",
        "    data_transform = None\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "train_dataset = ChangeDetectionDataset(PATH_TO_DATASET, train = True, stride = TRAIN_STRIDE, transform=data_transform)\n",
        "#weights = torch.FloatTensor(train_dataset.weights)\n",
        "weights = torch.FloatTensor(train_dataset.weights).cuda()\n",
        "print(weights)\n",
        "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)\n",
        "\n",
        "test_dataset = ChangeDetectionDataset(PATH_TO_DATASET, train = False, stride = TRAIN_STRIDE)\n",
        "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)\n",
        "\n",
        "\n",
        "print('DATASETS OK')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 14/14 [02:03<00:00,  8.83s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([0.0460, 1.9540], device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 10/10 [01:20<00:00,  8.04s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATASETS OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xT84TwlE22x"
      },
      "source": [
        "# print(weights) "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGrGMeR8E22y",
        "outputId": "ccda1a6b-5424-4787-8b3f-cef1864d6a66"
      },
      "source": [
        "# 0-RGB | 1-RGBIr | 2-All bands s.t. resulution <= 20m | 3-All bands\n",
        "\n",
        "if TYPE == 0:\n",
        "#     net, net_name = Unet(2*3, 2), 'FC-EF'\n",
        "  net, net_name = SiamUnet_conc(3, 2), 'FC-Siam-conc'\n",
        "#     net, net_name = SiamUnet_diff(3, 2), 'FC-Siam-diff'\n",
        "#     net, net_name = FresUNet(2*3, 2), 'FresUNet'\n",
        "elif TYPE == 1:\n",
        "#     net, net_name = Unet(2*4, 2), 'FC-EF'\n",
        "  net, net_name = SiamUnet_conc(4, 2), 'FC-Siam-conc'\n",
        "#     net, net_name = SiamUnet_diff(4, 2), 'FC-Siam-diff'\n",
        "#     net, net_name = FresUNet(2*4, 2), 'FresUNet'\n",
        "elif TYPE == 2:\n",
        "#     net, net_name = Unet(2*10, 2), 'FC-EF'\n",
        "  net, net_name = SiamUnet_conc(10, 2), 'FC-Siam-conc'\n",
        "#     net, net_name = SiamUnet_diff(10, 2), 'FC-Siam-diff'\n",
        "#     net, net_name = FresUNet(2*10, 2), 'FresUNet'\n",
        "elif TYPE == 3:\n",
        "#     net, net_name = Unet(2*13, 2), 'FC-EF'\n",
        "  net, net_name = SiamUnet_conc(13, 2), 'FC-Siam-conc'\n",
        "#     net, net_name = SiamUnet_diff(13, 2), 'FC-Siam-diff'\n",
        "#     net, net_name = FresUNet(2*13, 2), 'FresUNet'\n",
        "\n",
        "\n",
        "net.cuda()\n",
        "\n",
        "criterion = nn.NLLLoss(weight=weights) # to be used with logsoftmax output - need to think about tweaking this too.\n",
        "\n",
        "print('NETWORK OK')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NETWORK OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55bIf7GtE22y",
        "outputId": "4c545979-a32b-44e0-8296-cf969bc8d487"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('Number of trainable parameters:', count_parameters(net))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of trainable parameters: 1546130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XihBjBIZE22y"
      },
      "source": [
        "### This cell gives the procedure to train the model on the training dataset, and output \n",
        "### graphs that show the progress of the training through the epochs e.g. loss, recall etc.\n",
        "### Uses the Adam optimiser.\n",
        "### There are lots of things we could tweak here - optimiser, learning rate, weight decay (regularisation),\n",
        "### no. of epochs, as well as tweaking the fundamental structure of the ConvNet models used.\n",
        "\n",
        "# net.load_state_dict(torch.load('net-best_epoch-1_fm-0.7394933126157746.pth.tar'))\n",
        "\n",
        "def train(n_epochs = N_EPOCHS, save = True):\n",
        "    t = np.linspace(1, n_epochs, n_epochs)\n",
        "    \n",
        "    epoch_train_loss = 0 * t\n",
        "    epoch_train_accuracy = 0 * t\n",
        "    epoch_train_change_accuracy = 0 * t\n",
        "    epoch_train_nochange_accuracy = 0 * t\n",
        "    epoch_train_precision = 0 * t\n",
        "    epoch_train_recall = 0 * t\n",
        "    epoch_train_Fmeasure = 0 * t\n",
        "    epoch_test_loss = 0 * t\n",
        "    epoch_test_accuracy = 0 * t\n",
        "    epoch_test_change_accuracy = 0 * t\n",
        "    epoch_test_nochange_accuracy = 0 * t\n",
        "    epoch_test_precision = 0 * t\n",
        "    epoch_test_recall = 0 * t\n",
        "    epoch_test_Fmeasure = 0 * t\n",
        "    \n",
        "#     mean_acc = 0\n",
        "#     best_mean_acc = 0\n",
        "    fm = 0\n",
        "    best_fm = 0\n",
        "    \n",
        "    lss = 1000\n",
        "    best_lss = 1000\n",
        "    \n",
        "    plt.figure(num=1)\n",
        "    plt.figure(num=2)\n",
        "    plt.figure(num=3)\n",
        "    \n",
        "    \n",
        "    optimizer = torch.optim.Adam(net.parameters(), weight_decay=1e-4)\n",
        "#     optimizer = torch.optim.Adam(net.parameters(), lr=0.0005)\n",
        "        \n",
        "    \n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n",
        "    \n",
        "    \n",
        "    for epoch_index in tqdm(range(n_epochs)):\n",
        "        net.train()\n",
        "        print('Epoch: ' + str(epoch_index + 1) + ' of ' + str(N_EPOCHS))\n",
        "\n",
        "        tot_count = 0\n",
        "        tot_loss = 0\n",
        "        tot_accurate = 0\n",
        "        class_correct = list(0. for i in range(2))\n",
        "        class_total = list(0. for i in range(2))\n",
        "#         for batch_index, batch in enumerate(tqdm(data_loader)):\n",
        "        for batch in train_loader:\n",
        "            I1 = Variable(batch['I1'].float().cuda())\n",
        "            I2 = Variable(batch['I2'].float().cuda())\n",
        "            label = torch.squeeze(Variable(batch['label'].cuda()))\n",
        "            \n",
        "            #I1 = Variable(batch['I1'].float())\n",
        "            #I2 = Variable(batch['I2'].float())\n",
        "            #label = torch.squeeze(Variable(batch['label']))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = net(I1, I2)\n",
        "            loss = criterion(output, label.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "        epoch_train_loss[epoch_index], epoch_train_accuracy[epoch_index], cl_acc, pr_rec = test(train_dataset)\n",
        "        epoch_train_nochange_accuracy[epoch_index] = cl_acc[0]\n",
        "        epoch_train_change_accuracy[epoch_index] = cl_acc[1]\n",
        "        epoch_train_precision[epoch_index] = pr_rec[0]\n",
        "        epoch_train_recall[epoch_index] = pr_rec[1]\n",
        "        epoch_train_Fmeasure[epoch_index] = pr_rec[2]\n",
        "        \n",
        "#         epoch_test_loss[epoch_index], epoch_test_accuracy[epoch_index], cl_acc, pr_rec = test(test_dataset)\n",
        "        epoch_test_loss[epoch_index], epoch_test_accuracy[epoch_index], cl_acc, pr_rec = test(test_dataset)\n",
        "        epoch_test_nochange_accuracy[epoch_index] = cl_acc[0]\n",
        "        epoch_test_change_accuracy[epoch_index] = cl_acc[1]\n",
        "        epoch_test_precision[epoch_index] = pr_rec[0]\n",
        "        epoch_test_recall[epoch_index] = pr_rec[1]\n",
        "        epoch_test_Fmeasure[epoch_index] = pr_rec[2]\n",
        "\n",
        "        plt.figure(num=1)\n",
        "        plt.clf()\n",
        "        l1_1, = plt.plot(t[:epoch_index + 1], epoch_train_loss[:epoch_index + 1], label='Train loss')\n",
        "        l1_2, = plt.plot(t[:epoch_index + 1], epoch_test_loss[:epoch_index + 1], label='Test loss')\n",
        "        plt.legend(handles=[l1_1, l1_2])\n",
        "        plt.grid()\n",
        "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
        "        plt.gcf().gca().set_xlim(left = 0)\n",
        "        plt.title('Loss')\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "\n",
        "        plt.figure(num=2)\n",
        "        plt.clf()\n",
        "        l2_1, = plt.plot(t[:epoch_index + 1], epoch_train_accuracy[:epoch_index + 1], label='Train accuracy')\n",
        "        l2_2, = plt.plot(t[:epoch_index + 1], epoch_test_accuracy[:epoch_index + 1], label='Test accuracy')\n",
        "        plt.legend(handles=[l2_1, l2_2])\n",
        "        plt.grid()\n",
        "        plt.gcf().gca().set_ylim(0, 100)\n",
        "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
        "#         plt.gcf().gca().set_xlim(left = 0)\n",
        "        plt.title('Accuracy')\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "\n",
        "        plt.figure(num=3)\n",
        "        plt.clf()\n",
        "        l3_1, = plt.plot(t[:epoch_index + 1], epoch_train_nochange_accuracy[:epoch_index + 1], label='Train accuracy: no change')\n",
        "        l3_2, = plt.plot(t[:epoch_index + 1], epoch_train_change_accuracy[:epoch_index + 1], label='Train accuracy: change')\n",
        "        l3_3, = plt.plot(t[:epoch_index + 1], epoch_test_nochange_accuracy[:epoch_index + 1], label='Test accuracy: no change')\n",
        "        l3_4, = plt.plot(t[:epoch_index + 1], epoch_test_change_accuracy[:epoch_index + 1], label='Test accuracy: change')\n",
        "        plt.legend(handles=[l3_1, l3_2, l3_3, l3_4])\n",
        "        plt.grid()\n",
        "        plt.gcf().gca().set_ylim(0, 100)\n",
        "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
        "#         plt.gcf().gca().set_xlim(left = 0)\n",
        "        plt.title('Accuracy per class')\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "\n",
        "        plt.figure(num=4)\n",
        "        plt.clf()\n",
        "        l4_1, = plt.plot(t[:epoch_index + 1], epoch_train_precision[:epoch_index + 1], label='Train precision')\n",
        "        l4_2, = plt.plot(t[:epoch_index + 1], epoch_train_recall[:epoch_index + 1], label='Train recall')\n",
        "        l4_3, = plt.plot(t[:epoch_index + 1], epoch_train_Fmeasure[:epoch_index + 1], label='Train Dice/F1')\n",
        "        l4_4, = plt.plot(t[:epoch_index + 1], epoch_test_precision[:epoch_index + 1], label='Test precision')\n",
        "        l4_5, = plt.plot(t[:epoch_index + 1], epoch_test_recall[:epoch_index + 1], label='Test recall')\n",
        "        l4_6, = plt.plot(t[:epoch_index + 1], epoch_test_Fmeasure[:epoch_index + 1], label='Test Dice/F1')\n",
        "        plt.legend(handles=[l4_1, l4_2, l4_3, l4_4, l4_5, l4_6])\n",
        "        plt.grid()\n",
        "        plt.gcf().gca().set_ylim(0, 1)\n",
        "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
        "#         plt.gcf().gca().set_xlim(left = 0)\n",
        "        plt.title('Precision, Recall and F-measure')\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "        \n",
        "        \n",
        "#         mean_acc = (epoch_test_nochange_accuracy[epoch_index] + epoch_test_change_accuracy[epoch_index])/2\n",
        "#         if mean_acc > best_mean_acc:\n",
        "#             best_mean_acc = mean_acc\n",
        "#             save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_acc-' + str(mean_acc) + '.pth.tar'\n",
        "#             torch.save(net.state_dict(), save_str)\n",
        "        \n",
        "        \n",
        "#         fm = pr_rec[2]\n",
        "        fm = epoch_train_Fmeasure[epoch_index]\n",
        "        if fm > best_fm:\n",
        "            best_fm = fm\n",
        "            save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_fm-' + str(fm) + '.pth.tar'\n",
        "            torch.save(net.state_dict(), save_str)\n",
        "        \n",
        "        lss = epoch_train_loss[epoch_index]\n",
        "        if lss < best_lss:\n",
        "            best_lss = lss\n",
        "            save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_loss-' + str(lss) + '.pth.tar'\n",
        "            torch.save(net.state_dict(), save_str)\n",
        "            \n",
        "            \n",
        "#         print('Epoch loss: ' + str(tot_loss/tot_count))\n",
        "        if save:\n",
        "            im_format = 'png'\n",
        "    #         im_format = 'eps'\n",
        "\n",
        "            plt.figure(num=1)\n",
        "            plt.savefig(net_name + '-01-loss.' + im_format)\n",
        "\n",
        "            plt.figure(num=2)\n",
        "            plt.savefig(net_name + '-02-accuracy.' + im_format)\n",
        "\n",
        "            plt.figure(num=3)\n",
        "            plt.savefig(net_name + '-03-accuracy-per-class.' + im_format)\n",
        "\n",
        "            plt.figure(num=4)\n",
        "            plt.savefig(net_name + '-04-prec-rec-fmeas.' + im_format)\n",
        "        \n",
        "    out = {'train_loss': epoch_train_loss[-1],\n",
        "           'train_accuracy': epoch_train_accuracy[-1],\n",
        "           'train_nochange_accuracy': epoch_train_nochange_accuracy[-1],\n",
        "           'train_change_accuracy': epoch_train_change_accuracy[-1],\n",
        "           'test_loss': epoch_test_loss[-1],\n",
        "           'test_accuracy': epoch_test_accuracy[-1],\n",
        "           'test_nochange_accuracy': epoch_test_nochange_accuracy[-1],\n",
        "           'test_change_accuracy': epoch_test_change_accuracy[-1]}\n",
        "    \n",
        "    print('pr_c, rec_c, f_meas, pr_nc, rec_nc')\n",
        "    print(pr_rec)\n",
        "    \n",
        "    return out\n",
        "\n",
        "L = 1024\n",
        "N = 2\n",
        "\n",
        "def test(dset):\n",
        "    net.eval()\n",
        "    tot_loss = 0\n",
        "    tot_count = 0\n",
        "    tot_accurate = 0\n",
        "    \n",
        "    n = 2\n",
        "    class_correct = list(0. for i in range(n))\n",
        "    class_total = list(0. for i in range(n))\n",
        "    class_accuracy = list(0. for i in range(n))\n",
        "    \n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    \n",
        "    for img_index in dset.names:\n",
        "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
        "        \n",
        "        s = cm_full.shape\n",
        "        \n",
        "\n",
        "        steps0 = np.arange(0,s[0],ceil(s[0]/N))\n",
        "        steps1 = np.arange(0,s[1],ceil(s[1]/N))\n",
        "        for ii in range(N):\n",
        "            for jj in range(N):\n",
        "                xmin = steps0[ii]\n",
        "                if ii == N-1:\n",
        "                    xmax = s[0]\n",
        "                else:\n",
        "                    xmax = steps0[ii+1]\n",
        "                ymin = jj\n",
        "                if jj == N-1:\n",
        "                    ymax = s[1]\n",
        "                else:\n",
        "                    ymax = steps1[jj+1]\n",
        "                I1 = I1_full[:, xmin:xmax, ymin:ymax]\n",
        "                I2 = I2_full[:, xmin:xmax, ymin:ymax]\n",
        "                cm = cm_full[xmin:xmax, ymin:ymax]\n",
        "                \n",
        "                I1 = Variable(torch.unsqueeze(I1, 0).float()).cuda()\n",
        "                I2 = Variable(torch.unsqueeze(I2, 0).float()).cuda()\n",
        "                cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm),0).float()).cuda()\n",
        "\n",
        "\n",
        "                output = net(I1, I2)\n",
        "                loss = criterion(output, cm.long())\n",
        "        #         print(loss)\n",
        "                tot_loss += loss.data * np.prod(cm.size())\n",
        "                tot_count += np.prod(cm.size())\n",
        "\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "                c = (predicted.int() == cm.data.int())\n",
        "                for i in range(c.size(1)):\n",
        "                    for j in range(c.size(2)):\n",
        "                        l = int(cm.data[0, i, j])\n",
        "                        class_correct[l] += c[0, i, j]\n",
        "                        class_total[l] += 1\n",
        "                        \n",
        "                pr = (predicted.int() > 0).cpu().numpy()\n",
        "                gt = (cm.data.int() > 0).cpu().numpy()\n",
        "                \n",
        "                tp += np.logical_and(pr, gt).sum()\n",
        "                tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
        "                fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
        "                fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
        "        \n",
        "    net_loss = tot_loss/tot_count\n",
        "    net_accuracy = 100 * (tp + tn)/tot_count\n",
        "    \n",
        "    for i in range(n):\n",
        "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
        "\n",
        "    prec = tp / (tp + fp)\n",
        "    rec = tp / (tp + fn)\n",
        "    f_meas = 2 * prec * rec / (prec + rec)\n",
        "    prec_nc = tn / (tn + fn)\n",
        "    rec_nc = tn / (tn + fp)\n",
        "    \n",
        "    pr_rec = [prec, rec, f_meas, prec_nc, rec_nc]\n",
        "        \n",
        "    return net_loss, net_accuracy, class_accuracy, pr_rec\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "FUUR7CGLE22z",
        "outputId": "bb2719d5-6036-42f0-f1fe-a06b613cba96"
      },
      "source": [
        "### This cell either loads trained weights, or it begins the training process of a network by calling train().\n",
        "\n",
        "if LOAD_TRAINED:\n",
        "    net.load_state_dict(torch.load('net_final.pth.tar'))\n",
        "    print('LOAD OK')\n",
        "else:\n",
        "    t_start = time.time()\n",
        "    out_dic = train()\n",
        "    t_end = time.time()\n",
        "    print(out_dic)\n",
        "    print('Elapsed time:')\n",
        "    print(t_end - t_start)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU1f34/9c7CUmAxLAIyFpiZQshCwQQUQgqgksFUeuCWm0VaT/Wth8XaFVqrfrTflttRSvSalE/WrFYl1ZRq5KCILIoskUWIUIAFQKEBEzIcn5/3DvD7LmZzGSSyfv5eOQxM/ece+773HvnnTN3Zs6IMQallFKtX0KsA1BKKRUZmtCVUipOaEJXSqk4oQldKaXihCZ0pZSKE5rQlVIqTmhCb+NEZLqIvOug3jwRuac5YmpuIrJARO637xeKSGlzb1epSNCE3oKJSImIfCsilSLytZ0A0iK5DWPMC8aY8xzUm2mM+W0ktx2IiBSJSJXd5wMi8k8R6Rnt7bY0InK9iNTZ+8H193is41Itmyb0lu97xpg0YDhQANztW0FEkpo9qui6xe7zaUAa8PsYxxMrHxlj0jz+bol1QJEiIomxjiEeaUJvJYwxe4DFQDaAiBgR+R8R2QZss5ddJCLrROSwiKwQkRzX+iLS1x7t7heRMtdozx4JfmjfFxF5VES+EZEjIrJBRFzb87o8ICI3ich2ETkoIm+ISC+PMiMiM0Vkmx3LEyIiYfT5MPAakOfR9mAR+Y+93S0i8n2PsvYi8gcR+VJEykXkQxFpb5f9Q0S+spcvFZGhjY3HbudPIrLb3j9rReQsj7J7ReRlEXlORCpEZJOIFHiU54vIJ3bZQiA1nBgCxFQoIqUicqd97PaJyFQRuUBEttr76lce9RNEZLaIfGGfCy+LSBeP8qD7ym5zs92HPSJyu73cfR551DUicpp9f4GIPCkib4nIUWCCiPQSkVfsc3KniNwaif3RlmlCbyVEpC9wAfCpx+KpwGggS0TygWeAm4GuwFPAGyKSYo+G/g18CfQHegMvBdjMecA4YCCQAXwfKAsQy9nA/2eX97Tb9W3vImAkkGPXmxRGn7sC04Dt9uOOwH+AF4HuwJXAn0Uky17l98AI4AygC3AnUG+XLQYG2Ot9ArzQ2Hhsq7H+wXSx4/iHiHgm5oux9kUn4A3A9Y8zGeuf0/P2uv8ALg0zhkBOwfoH0RuYA/wFuAZrf5wF3CMimXbdn2KdO+OBXsAh4AmPtkLtq6eBm40x6ViDiw8aEePVwANAOrAC+BfwmR3zOcDPRaTR54nyYIzRvxb6B5QAlcBhrKT5Z6C9XWaAsz3qPgn81mf9LVhP2jHAfiApwDauBz60758NbAVOBxJ86i0A7rfvPw38zqMsDagB+nvEdqZH+cvAbId9LgKOAeV2O+uAfnbZFcAyn/pPAb/GGpx8C+Q62EYnu+2MAH0rBEobcYwOubYJ3Au851GWBXxr3x8H7AXEo3yFa7tBjkutfexdf6cHqVto9z3Rfpxu92+0R521wFT7fjFwjkdZT/v4BTo/fPfVLqxBw0nBziOPZQY4zWMfP+dRNhrY5VP/l8DfYv28a81/OkJv+aYaYzoZY75jjPmJMeZbj7LdHve/A9xmX+I4LCKHgb5YI7C+wJfGmNpQGzLGfIA1onwC+EZE5ovISQGq9sL6B+NarxJrJN/bo85XHvePYSV9p241xmRgje47A308+jjap4/TsUanJ2ONUL/wbUxEEkXkIfsSwxGsf5TY6zSKiNwuIsX25YjDWK9kPNvx7XeqWO9x9AL2GDtz2b4ktJX2sXf9rRSRfuLxRqlH3TJjTJ1933WOfO1R/i0njsF3gFc99mExUAf0cLCvLsV6pfiliPxXRMY00AdPvudrL59j+SugRyPaUz40obdunslhN/CATwLoYIz5u13WTxy8eWqMecwYMwJrdDkQuCNAtb1YT0jAfSmkK7CnCX0JFMsG4H7AdQ1+N/Bfnz6mGWN+DBwAqoDvBmjqamAKcC5WAu7vCr0x8djXy+/EuoTU2RjTCeuVhJN29gG9fd5L6NeY7QMYY3YZjzdKG7u+bTdwvs9+TDXW+zQh95UxZrUxZgrW5ZjXsF59ARwFOrg2ICKnBArfJ4adPjGkG2MuCLNPCk3o8eQvwEwRGS2WjiJyoYikA6uwEspD9vJUERnr24CIjLTXb4f1BK3ixDVoT38HbhCRPBFJAR4EPjbGlDQUpIj0t98s6++wX89ijdouxnofYKCIXCsi7ey/kSIyxBhTj/UewiP2m22JIjLGji8dqMZ6FdHBjjcc6ViXQfYDSSIyBwj0CiaQj+x1b7XjngaMCjOOppoHPCAi3wEQkW4iMsUuC7qvRCRZrO8tZBhjaoAjnDg/PgOG2udEKtblp1BWARUiMkusN7MTRSRbREZGqpNtkSb0OGGMWQPchHXJ5BDWG4nX22V1wPewPga4CyjFuh7t6ySsfwyHsC4HlAH/L8C23gPuAV7B+kfxXaw3KJ3oa7ftaDRvjDkO/Am4xxhTgfXG7ZVYrxK+Ah4GUuzqtwMbsN64PGiXJQDPeWxzM7DSYay+3gHexnqf4Uusf3i7Q67h3Y9pWMfkINb+/2eYcTTVn7DesH1XRCqw9sdou6yhfXUtUGJfjpmJdckLY8xW4D7gPaxPXX1ICPY5eRHWG8w7sV5h/RXrVYEKk3hf0lMqukTkbmC/MeapWMeiVLzRhK6UUnGiwUsuIvKMWF9W2BikXETkMbG+ZLJeRIZHPkyllFINcXINfQEwOUT5+VhfQhgAzMD6PLRSSqlm1mBCN8YsxXoTJ5gpWF8YMMaYlUAnaYOTKSmlVKxFYlKn3ni/019qL9vnW1FEZmCN4mnfvv2Ivn37hrXB+vp6EhLa1gd0tM9tg/a5bWhKn7du3XrAGNMtUFmzztJnjJkPzAcoKCgwa9asCaudoqIiCgsLIxhZy6d9bhu0z21DU/osIkG/YRyJf4t7sD5b7NKHCH9jUCmlVMMikdDfAK6zP+1yOlBujPG73KKUUiq6GrzkIiJ/x5rN7WSxfprr10A7AGPMPOAtrMl6tmNNRnRDtIJVSikVnJPJmq5qoNwA/xOxiJRSSoWlbb21rJRScUwTulJKxQlN6EopFSc0oSulVJzQhK6UUnFCE7pSSsUJTehKKRUnNKErpVSc0ISulFJxQhO6UkrFCU3oSikVJzShK6VUnNCErpRScUITulJKxQlN6EopFSc0oSulVJzQhK6UUnFCE7pSSsUJTehKKRUnNKErpVSc0ISulFJxQhO6UkrFCU3oSikVJzShK6VUnNCErpRScUITulJKxQlN6EopFSeSYh2AUkq1CPX1YOqgvtb+q7P/aoMvdz0OVe4uO3Hb4ahEpQua0JVqi4xxmIh8y2udJTlT71EWOsn13/kF1P43SHKstRKt52OvtoPFFOC2oXYxzbb7Ow2YGZV2NaGrtsHzyRv0SR/oie+bTJwmOd96DSQ5e71Be0vh4IthjACdlntsr4XoD7C7HSQkQUKi/Zdk/Uli4OUJiR5l9uOkZJ/1fMrDajcJEhK8H3u17XPrG5PfNq3HX6/ZyMAo7EtN6PHIGAeJquGXhc5HTAESRgRequYcPAAlJwVOno1qtzbWR8RbiITR+XgtVHVoOBElpUBCxwYSim8i8nns2bYESVBOklzQdZ3ElEjR0qUUFhbG+qg0q7qkL6LSbutP6MY04iVYI0ZMQROVgyTXqFGcT9t+MVmPR1Uegc+SnSVPUx/ro3KC+D7xnY9sEuuqrb4kJEFSqqORj1/boUZMfkkuWExOkpzDmCQBJPj105VFRW0uuanIaX0J/aM/c9bS38Ay0+JeOgL+I6AGRzYBnvTt2vsllErK6HBKryiMmJwkIicvVYOMAEMkr4Z8qslNqUZpfQn9lGz29ppM3+/0j8yIKWIvVe31omRzURHdNbkppUJofQk9cxxfnFZPX01uSinlxdGQUkQmi8gWEdkuIrMDlPcTkSUi8qmIrBeRCyIfqlJKqVAaTOgikgg8AZwPZAFXiUiWT7W7gZeNMfnAlcCfIx2oUkqp0JyM0EcB240xO4wxx4GXgCk+dQxwkn0/A9gbuRCVUko5IcaE/naUiFwGTDbG3Gg/vhYYbYy5xaNOT+BdoDPQETjXGLM2QFszgBkAPXr0GPHSSy+FFXRlZSVpaWlhrdtaaZ/bBu1z29CUPk+YMGGtMaYgUFmk3hS9ClhgjPmDiIwBnheRbGO8PxBtjJkPzAcoKCgw4X4kragNfpxN+9w2aJ/bhmj12ckllz1AX4/Hfexlnn4EvAxgjPkISAVOjkSASimlnHGS0FcDA0QkU0SSsd70fMOnzi7gHAARGYKV0PdHMlCllFKhNZjQjTG1wC3AO0Ax1qdZNonIfSJysV3tNuAmEfkM+DtwvWno4rxSSqmIcnQN3RjzFvCWz7I5Hvc3A2MjG5pSSqnGaH3fFFVKqRbCGIOpN9acevWG+nrrcX2dwRj7NsDy2uroXMDQhK6UalDIxBUkkbmX1RuMV4LzbqN8t2Hb6q+969cHaMNvOe62641961HfFYvvcq9yVxuh+hRguWv7pj68xNyzQH+xSKmoi2Ti8ks6QZPKiWUHthnWVpVEJHH5jxIJPXqMQuJyqnT5pkbVFwFJFBJErNsEQRKs24REQYIux7q1l0mSeD121ZeEwMut2xNt+G7Tq22f5Z5tbN+zMSr7URN6G+VKXA0mnSglruCjIdyJa+/uepaUfh6RxOU7wgvep1gfGfj6sx3u++7EFSJ5BFzul7gSGp+4EoWEBPwTlF+CC564xCMW/7atx2vXrmHU6FEn+hAwDjuRuuJuwrTMLcGuch2hh6UxiStQIgqcPIhc4vJc16eu5/KyA/W8vu7TuEpcngIlrto6qP7mgIPRECeSip24/JNK5BKX37IASSfYKNGrDwHaWL58GePGj4ubxOVE6hdCl54dYx1GXGh1CX3z8r1se7Oe3e9/5H1drrUkLs8nfZDkEShx1VVD7fE6x4nLL9lIkKQTKDlFIHGFfAma4N1GsMRlfZvuzBgcpdhJSBKS2iXGOgzVSrW6hN4+rR2pnaFHj5OanLhCvnxMiFzicrXTlBGXldwCTt+glFJAK0zombnd+PJQAoWFQ2MdilJKtSjR+800pZRSzUoTulJKxQlN6EopFSc0oSulVJzQhK6UUnGi1X3KRSkVvpqaGkpLS6mqqop1KG4ZGRkUFxfHOoxm5aTPqamp9OnTh3bt2jluVxO6Um1IaWkp6enp9O/fv8V8C7WiooL09PRYh9GsGuqzMYaysjJKS0vJzMx03K5eclGqDamqqqJr164tJpmrwESErl27NvqVlCZ0pdoYTeatQzjHSRO6UqrZlJWVkZeXR15eHqeccgq9e/dm7Nix5OXlcfz48ZDrrlmzhltvvbWZIg1u7969XHbZZSHrnHHGGc0UjTe9hq6UajZdu3Zl3bp1ANx7772kpaVx8803u68n19bWkpQUOC0VFBRQUBD5+YxCbTOQXr16sWjRopB1VqxY0dSwwqIjdKVUTM2cOZOZM2cyevRo7rzzTlatWsWYMWPIz8/njDPOYMuWLYA1Qd1FF10EWP8MfvjDH1JYWMipp57KY489FrDttLQ0fvGLXzB06FDOOecc9u/fD0BhYSE///nPKSgo4E9/+hNr165l/PjxjBgxgkmTJrFv3z4Atm/fzrnnnktubi7Dhw/niy++oKSkhOzsbAA2bdrEqFGjyMvLIycnh23btrm3C9abm3fccQfZ2dkMGzaMhQsXArBs2TIKCwu57LLLGDx4MNOnT8dEYGpYHaEr1Ub95l+b2Lz3SETbzOp1Er/+XuMnzistLWXFihUkJiZy5MgRli1bRlJSEu+99x6/+tWveOWVV/zW+fzzz1myZAkVFRUMGjSIH//4x34f8Tt69CgFBQU8+uij3HffffzmN7/h8ccfB+D48eOsWbOGmpoaxo8fz+uvv063bt1YuHAhd911F8888wzTp09n9uzZXHLJJVRVVVFfX88333zjbn/evHn87Gc/Y/r06Rw/fpy6ujqv7f/zn/9k3bp1fPbZZxw4cICRI0cybtw4AD799FM2bdpEr169GDt2LMuXL+fMM5s2XbQmdKVUzF1++eUkJlrzwJeXl/ODH/yAbdu2ISLU1NQEXOfCCy8kJSWFlJQUunfvztdff02fPn286iQkJHDFFVcAcM011zBt2jR3mWv5li1b2LhxIxMnTgSgrq6Onj17UlFRwZ49e7jkkksA63PhvsaMGcMDDzxAaWkp06ZNY8CAAV7lH374IVdddRWJiYn06NGD8ePHs3r1apKSkhg1apQ73ry8PEpKSjShK6XCE85IOlo6djzxi0X33HMPEyZM4NVXX6WkpITCwsKA66SkpLjvJyYmUltb2+B2PD854tqmMYahQ4fy0UcfedWtqKhosL2rr76a0aNH8+abb3LBBRfw1FNPcfbZZze4XrjxN0SvoSulWpTy8nJ69+4NwIIFC5rUVn19vfsNzBdffDHgCHjQoEHs37/fndBramrYtGkT6enp9OnTh9deew2A6upqjh075rXujh07OPXUU7n11luZMmUK69ev9yo/66yzWLhwIXV1dezfv5+lS5cyatSoJvUpFE3oSqkW5c477+SXv/wl+fn5TR61duzYkVWrVpGdnc0HH3zAnDlz/OokJyezaNEiZs2aRW5uLnl5ee5PqTz//PM89thj5OTkcMYZZ/DVV195rfvyyy+TnZ1NXl4eGzdu5LrrrvMqv+SSS8jJySE3N5ezzz6b3/3ud5xyyilN6lMoEol3VsNRUFBg1qxZE9a61s+xFUY2oBZO+9w2RLvPxcXFDBkyJGrthyOaX/1PS0ujsrIyKm03hdM+BzpeIrLWGBPw85s6QldKqTihCV0pFbda4ug8mjShK6VUnNCErpRScUITulJKxQlN6EopFSc0oSulmk08TJ/rOUnYggULuOWWW2Ic0Qn61X+lVLOJ5fS5dXV17vli4pWjEbqITBaRLSKyXURmB6nzfRHZLCKbROTFyIaplIpX0Z4+97bbbiM3N5ePPvqI//u//3NPd3vzzTe7Z0d8++23GT58OLm5uZxzzjkAQeNoyRocoYtIIvAEMBEoBVaLyBvGmM0edQYAvwTGGmMOiUj3aAWslIqQxbPhqw2RbfOUYXD+Q41eLZrT544ePZo//OEPFBcX8/DDD7N8+XLatWvHT37yE1544QXOP/98brrpJpYuXUpmZiYHDx4EYPDgwY7iaEmcXHIZBWw3xuwAEJGXgCnAZo86NwFPGGMOARhjvvFrRSmlgojW9LmJiYlceumlALz//vusXbuWkSNHAvDtt9/SvXt3Vq5cybhx48jMzASgS5cujYqjJXGS0HsDuz0elwKjfeoMBBCR5UAicK8x5m3fhkRkBjADoEePHhQVFYURsvXtr3DXba20z21DtPuckZFxYlrYM++KzkYcTDsL1uyF7dq1wxhDQkKCO67Zs2czZswYnnvuOb788ksuvPBCKioqOHbsGLW1tVRUVLjXda0jIhw+fJiMjAyvbaSmprpnSPz222+56qqruPfee73qLF68mJqaGr/pcp3EUVVVxfHjxx1Nteuprq7O0TpVVVWNOh8i9aZoEjAAKAT6AEtFZJgx5rBnJWPMfGA+WJNzhTsJkU7a1DZonyOvuLg4ahNhNZZrdC0itG/f3h3XsWPH+O53v0t6ejqLFi1CREhPT6dDhw4kJSWRnp7uXte1TkJCAmlpaQH75lp24YUXMmXKFGbNmkX37t05ePAgFRUVTJgwgdtuu40DBw64L7l06dLFURypqakkJyc3ep86nZwrNTWV/Px8x+06eVN0D9DX43Efe5mnUuANY0yNMWYnsBUrwSulVKNEcvpcT1lZWdx///2cd9555OTkMHHiRPbt20e3bt2YP38+06ZNIzc31/1LRtGKI6qMMSH/sEbfO4BMIBn4DBjqU2cy8Kx9/2SsSzRdQ7U7YsQIE64lS5aEvW5rpX1uG6Ld582bN0e1/XAcOXIk1iE0O6d9DnS8gDUmSF5tcIRujKkFbgHeAYqBl40xm0TkPhG52K72DlAmIpuBJcAdxpiyiP3XUUop1SBH19CNMW8Bb/ksm+Nx3wD/a/8ppZSKAf3qv1JKxQlN6EopFSc0oSulVJzQhK6UUnFCE7pSqtk09/S5/fv3Z9iwYQwbNoysrCzuvvtuqqqqANi7dy+XXXZZ2H1ZuXIlN910E0VFRWRkZLj7de655wKwdOlShg8fTlJSEosWLQp7O42h0+cqpZpNLKbPXbJkCSeffDKVlZXMmDGDm2++mWeffZZevXo1KdEuXryYyZMnA3DWWWfx73//26u8X79+LFiwgN///vdhb6OxdISulIqpaE6f6yktLY158+bx2muvcfDgQUpKSsjOzgasuVVuv/12srOzycnJYe7cuQCsXbuW8ePHM2LECCZNmsS+ffvc7b3//vvu0Xgg/fv3Jycnh4SE5kuzOkJXqo16eNXDfH7w84i2ObjLYGaNmtXo9aI1fa6vk046iczMTLZt20aPHj3cy+fPn09JSQnr1q0jKSmJgwcPUlNTw09/+lNef/11unXrxsKFC7nrrrt45plnOHDgAO3atXNPBrZs2TLy8vIAa+bIu+6K0sRnDdCErpSKuWhNnxuI9T1Ib++99x4zZ850X+7p0qULGzduZOPGjUycOBGwRvE9e/YE4N133+W8885zrx/okkssaEJXqo0KZyQdLR07dnTfv+eee5gwYQKvvvoqJSUlQWefTElJcd9PTEx0NIFWRUUFJSUlDBw4kPLy8pB1jTEMHTqUjz76yK9s8eLF/O//trwvxus1dKVUi1JeXk7v3r0B60eYI6WyspKf/OQnTJ06lc6dO3uVTZw4kaeeesr9T+HgwYMMGjSI/fv3uxN6TU0NmzZtwhjD+vXr3ZdYWhJN6EqpFiXS09ZOmDCB7OxsRo0aRb9+/Xjqqaf86tx4443069ePnJwccnNzefHFF0lOTmbRokXMmjWL3Nxc8vLyWLFiBWvXriU/Px8RCbnd1atX06dPH/7xj39w8803M3To0Cb3pSES6HpScygoKDBr1qwJa1394YO2QfscecXFxQwZMiRq7YfD6Y89tBT3338/p512GldeeWXYbTjtc6DjJSJrjTEBP7+p19CVUqoR7r777liHEJReclFKqTihCV0ppeKEJnSllIoTmtCVUipOaEJXSqk4oZ9yUUo1m7KyMs455xwAvvrqKxITE+natSsJCQmsWrWK5OTkkOsXFRWRnJzMGWec0RzhAjBnzhzGjRsXdCKuefPm0aFDB6677rpmiykYTehKqWbT0PS5DSkqKiItLS3shB5qet5g7rvvvpDlM2fODCuWaNBLLkqpmPr0008DTlH72GOPkZWVRU5ODldeeSUlJSXMmzePRx99lLy8PJYtW+bVzr333su1117LmDFjGDBgAH/5y18A65/AWWedxcUXX0xWVhZ1dXXccccdjBw5kpycHK9vjj788MMMGzaM3NxcZs+eDcD111/vnjd99uzZ7phuv/1293Zdc56vW7eO008/nZycHC655BIOHToEQGFhIbNmzWLUqFEMHDiQFStWRGVf6ghdqTbqqwcfpLo4stPnpgwZzCm/+pXj+sYY7rjjDv7973/7TVH70EMPsXPnTlJSUjh8+DCdOnVi5syZpKWluZOpr/Xr17Ny5UqOHj1Kfn4+F154IQCffPIJGzduJDMzk/nz55ORkcHq1auprq5m7NixnHfeeXz++ee8/vrrfPzxx3To0IGDBw96tV1WVsarr77K559/johw+PBhv+1fd911zJ07l/HjxzNnzhx+85vf8Mc//hGwXh2sWrWKt956i4ceeohJkyY53k9OaUJXSsVMdXU1xcXFAaeozcnJYfr06UydOpWpU6c6am/KlCm0b9+e9u3bM2HCBFatWkWnTp0YNWoUmZmZgDX17fr1692j7vLycrZt28Z7773HDTfcQIcOHQBrCl1PGRkZpKam8qMf/YiLLrrI/WMbLuXl5Rw+fJjx48cD8IMf/IDLL7/cXT5t2jQARowYwZdfftmo/eSUJnSl2qjGjKSjxRjD4MGDWbVqlV/Zm2++ydKlS/nXv/7FAw88wIYNGxpsz3fCLNdjz+l5jTHMnTvXb4T8zjvvhGw7KSmJVatW8f7777No0SIef/xxPvjggwZjcnFN95uYmEhdXZ3j9RpDr6ErpWImJSWFAwcO+E1RW19fz+7du5kwYQIPP/ww5eXlVFZWkp6eTkVFRdD2Xn/9daqqqigrK6OoqIiRI0f61Zk0aRJPPvmk+4cztm7dytGjR5k4cSJ/+9vfOHbsGIDfJZfKykrKy8u54IILePTRR/nss8+8yjMyMujcubP72v7zzz/vHq03Fx2hK6ViJiEhgeeff55Zs2ZRXl5ObW0tP//5zxk4cCDXXHMN5eXlGGO49dZb6dSpE9/73ve47LLLeP3115k7dy5nnXWWV3s5OTlMmDCBAwcOcM8999CrVy+2bt3qVefGG2+kpKSE4cOHY4yhW7duvPbaa0yePJl169ZRUFBAcnIyF1xwAQ8++KB7vYqKCqZMmUJVVRXGGB555BG//jz77LPMnDmTY8eOceqpp/K3v/0tOjsuCJ0+t5XQPrcNOn1u+Fwfgwz2hmlLEq3pc/WSi1JKxQm95KKUigv33ntvrEOIOR2hK6VUnNCErpRScUITulJKxQlN6EopFSc0oSulmk1ZWRl5eXnk5eVxyimn0Lt3b8aOHUteXh7Hjx9vcP2ioqKoTWwVTGFhIa6PWPfv358DBw406/Ybw9GnXERkMvAnIBH4qzHmoSD1LgUWASONMeF9yFwpFbeaa/rccKbJjQcNjtBFJBF4AjgfyAKuEpGsAPXSgZ8BH0c6SKVU/Ir09Lljx47l2muvZf/+/Vx66aWMHDmSkSNHsnz5csD6Cv8NN9zAsGHDyMnJ4ZVXXgHgxz/+MQUFBQwdOpRf//rXzbsTIsTJv7BRwHZjzA4AEXkJmAJs9qn3W+Bh4I6IRqiUioplL2/lwO7KiLZ5ctiMxAoAABRzSURBVN80zvr+QMf1Iz197ubNm/nwww9p3749V199Nb/4xS8488wz2bVrF5MmTaK4uJjf/va3ZGRkuCf7cs1Z/sADD9ClSxfq6uo455xzWL9+PTk5OU3fKc3ISULvDez2eFwKjPasICLDgb7GmDdFJGhCF5EZwAyAHj16UFRU1OiAwfoPG+66rZX2uW2Idp8zMjLck1vVHK+J+Kx/NcdrQk6e5am6upq6ujqKi4vdP0tXV1dHjx49qKioICsriyuuuIILL7yQiy66iMTERKqrq2nXrl3AbVRXVzNp0iRqa2upqKjgP//5Dxs3bnSXl5eXs2/fPt59912eeeYZdxtJSUlUVFTw3HPPsWDBAmpra/nqq69Yu3YtmZmZ1NXVcfToUSoqKjDGUFlZ6Z45MVx1dXWO9lNVVVWjzocmX2QSkQTgEeD6huoaY+YD88GayyXcOSt0jo+2QfscecXFxe7r1WdfMzRq23EiJSWFxMTEoNPnvvPOO+7pcx955BE2bNhASkoKKSkpAa+5p6SkkJaW5i4zxrBq1SpSU1O96iUkJHjVA9i5cyePP/44q1evpnPnzlx//fWICOnp6SQmJtKxY0fS09MREb91w+F0LpfU1FTy8/Mdt+vkUy57gL4ej/vYy1zSgWygSERKgNOBN0Qk4OQxSinlEunpcz2dd955zJ071/3Y9WbsxIkTeeKJJ9zLDx06xJEjR+jYsSMZGRl8/fXXLF68OIK9bD5OEvpqYICIZIpIMnAl8Iar0BhTbow52RjT3xjTH1gJXKyfclFKNcRz+tzc3Fzy8vJYsWIFdXV1XHPNNQwbNoz8/Hyv6XNfffXVgG+K+nrsscdYs2YNOTk5ZGVlMW/ePADuvvtuDh06RHZ2Nrm5uSxZsoTc3Fzy8/MZPHgwV199NWPHjm2O7kdcg5dcjDG1InIL8A7WxxafMcZsEpH7gDXGmDdCt6CUUv5ck2lVVFSwdOlSv/IPP/zQb9nAgQNZv359yPZcTj75ZBYuXOhXLy0tjWeffdZv+YIFCwK263kNu6SkJGCdlsLRNXRjzFvAWz7L5gSpW9j0sJRSSjWWflNUKaXihCZ0pZSKE5rQlVIqTmhCV0qpOKEJXSml4oQmdKVUs4nm9LkLFiygW7du5OfnM2DAACZNmuRVd86cObz33nthx37++edTWlpKYWEhgwYNcvdj0aJFAPzwhz+ke/fuZGdnh72Npmp780sqpWIm2tPnXnHFFTz++OMALFmyhGnTprFkyRKGDBnCfffdF3bc3377LWVlZfTp0weAF154gYIC7y/DX3/99dxyyy1cd911YW+nqXSErpSKqUhNn+trwoQJzJgxg/nz5wNWwnWNplevXs0ZZ5xBbm4uo0aNoqKigrq6Ou644w5GjhxJTk4OTz31lLstJ3PsjBs3ji5dujRhTzSdjtCVaqOWLJjPN1/uiGib3b9zKhOun+G4fqSnz/U1fPhwr8QMcPz4ca644goWLlzIyJEjOXLkCO3bt+fpp58mIyOD1atXU11dzdixYznvvPPIzMxk8eLFTJ061d3G9OnTad++PQDvv/8+Xbt2ddznaNKErpSKmerqaoqLi5k4cSJgTSvbs2dPAHJycpg+fTpTp071SqaNYYzxW7ZlyxZ69uzJyJEjATjppJMAePfdd1m/fr17FF9eXs62bdvIzMxk+fLl/P73v3e3EeiSS0ugCV2pNqoxI+loMcYEnT73zTffdE+f+8ADD7h/kKIxPv30U4YMGeI4lrlz5zJp0iSv5Tt27KBv374kJyc3evvNTa+hK6ViJprT5/73v/9l/vz53HTTTV7LBw0axL59+1i9ejVgTQ5WW1vLpEmTePLJJ6mpqQFg69atHD16lMWLFzN58uQI9jp6NKErpWIm0tPnLly4kLy8PAYOHMiDDz7IK6+84jdCT05OZuHChfz0pz8lNzeXiRMnUlVVxY033khWVhbDhw8nOzubm2++mdraWt5++21HCf2qq65izJgxbNmyhT59+vD0009HbD85ZoyJyd+IESNMuJYsWRL2uq2V9rltiHafN2/eHNX2w3HkyJFYhxBUVVWVaUquCsZpnwMdL6xpywPmVR2hK6VUECkpKaxZ03p+q0cTulJKxQlN6EopFSc0oSvVxpgAn81WLU84x0kTulJtSGpqKmVlZZrUWzhjDGVlZaSmpjZqPf1ikVJtSJ8+fSgtLWX//v2xDsWtqqqq0YmrtXPS59TUVPdkYE5pQleqDWnXrh2ZmZmxDsNLUVER+fn5sQ6jWUWrz3rJRSml4oQmdKWUihOa0JVSKk5oQldKqTihCV0ppeKEJnSllIoTmtCVUipOaEJXSqk4oQldKaXihCZ0pZSKE5rQlVIqTmhCV0qpOKEJXSml4oSjhC4ik0Vki4hsF5HZAcr/V0Q2i8h6EXlfRL4T+VCVUkqF0mBCF5FE4AngfCALuEpEsnyqfQoUGGNygEXA7yIdqFJKqdCcjNBHAduNMTuMMceBl4ApnhWMMUuMMcfshyuBxs3KrpRSqsmkoZ+iEpHLgMnGmBvtx9cCo40xtwSp/zjwlTHm/gBlM4AZAD169Bjx0ksvhRV0ZWUlaWlpYa3bWmmf2wbtc9vQlD5PmDBhrTGmIFBZRH+xSESuAQqA8YHKjTHzgfkABQUFprCwMKztFBUVEe66rZX2uW3QPrcN0eqzk4S+B+jr8biPvcyLiJwL3AWMN8ZURyY8pZRSTjm5hr4aGCAimSKSDFwJvOFZQUTygaeAi40x30Q+TKWUUg1pMKEbY2qBW4B3gGLgZWPMJhG5T0Qutqv9PyAN+IeIrBORN4I0p5RSKkocXUM3xrwFvOWzbI7H/XMjHJdSSvkxxoAxGAym3oB9azBQ77PcmBP1PW6tv3ow+C3HLrPuuurU29sEU19vxeFx671Nj/p+27QeYwzHK49EZf9E9E1R1bJ5PhlcJ6qTEzDwSe9T7vHE8nyiWbf1GCuAAG3UB4wFAxV7drFr43r/J1Yj+tBQv6LRh0DreScCjyTkuZ6pZ8+evby7dYPPdnz6HqhvIRJZ4/aJ3ZeACdO/Lyf2SaDjGDwZGrDbM9TU1LBhwROB1/NNhnGi37jojIFbXULftfEzdi9fwgclW3yeDE07qRrzHzZyowTXSRo4kXg+iaqrqih+8a+Bn3w+/XWV+e2DVmjrGy/HOoTGE0EQJEHAvhUEEnyWi/WHx21NTQ3f7t3lfuwq861r3U9AXNvzq5MAAiIJdjPWY1eZ57KEhEREsNoTsbqQYLUtCQnux94xu9YPFZ9967EPPPeJ63ZP6R769O3rt75nX0QI0p5nfCfi9l7Ps7+B+uDaT/7HQzz7Huw4upYH2Af+7Vn1N23fEZVTr9Ul9AO7Sjjw+QbKt3/e4EnvdfACnVTBTsCQ7dknid1OQkKCx4ENdFJ5r+d5AnmfML5PIu8TZt++ffTq1SvkkyhQXwI+uRz11+dJFLQP+D2xBMC3L4H63sBJ/9lnn5GXn+84GTrbJ876cKIvntt02Icm0I/wtQ1b934dlXZbXUIffsEUjnTIaHMnQFs86XccOETfrGGxDkOpVkNnW1RKqTihCV0ppeKEJnSllIoTmtCVUipOaEJXSqk4oQldKaXihCZ0pZSKE5rQlVIqTmhCV0qpONHqvimqVEvmmjjMfeuagAuf5fZEU751j9Udo7y63GuZezKyAO17LfNcHmibnst91gkUi+O6YcTiWXfDsQ3U76qPTNxB+uJe3tT9GoFjjIHO1Z0ppDDi51+rS+g7Du9gzdE1VO6obNUH1OnJ6drmnrI9LP1oqaO64TypGuxLkLij0m/7/tGjR/nja390vI/C6ncE91HEhPdTu63bklgHEFnimm8I18RduO+LCNMypkVlu60uoReVFvHsgWdhWawjCa2hA+q6DVnXY1lNTQ2bd212VDdgu/YtELiuz7JgdYP1xbNugmtSq2B1A+2LAMsPHD9A907dHe+jsPrtsK6Tfod7vD2Xf/HFFww4bUDDdRs6XmH0pbFxR+IYCMLatWspKCiITNwOYonEfm1oXzWkqKjIUb3GanUJ/dIBl9JxX0dGjxrd6AOKwTo8xrjWQox9615u1SfQcsOJdowAJ5a7t2PXPzFdLuAavbmmy3WXhyoznBj0GVZ9/DGjRoxssF13mTHudUO1G7TMntbXeZlxNxm0rL5x7RZ/UczgpMHefSTINhsq8y1vUpnxK3dPTxygLGi77nVPlO0p/YbevdoFKAu0zSBlru2C9/nQ1DLPVyJhrGt10b8st7ycDie9FbAs0Hr+86ObIH3hRMyNKQsRq/e64Ze1//7lEKMfiW5R6l56jWF/eIQqe4pT75MNv53n9bgVOxmIzgzKLVcGsC/WQThlT5/rdV/sf/Aejz3LA5Wl1tVS3i455HquwUvEy8QewgToR6S2KQG2aaqrSUhL81sP96g8SDyueCNYhkiQ8lBlDrfpHr0LZV06OzmrGq3VJfSUwUM4Nm4cffv189h3rp3lc3CsQu8T1ePgNLRuuGXubXqVhypzNerRrs+Ta3PxZoYOHepfFqhdd3mosoa36VfmdcKGKpMg5bjjclL28apVjD79dK9ycR27xrTre6z9ygK0y4nHQogy9/Yjo01Ok1xURF4b6/NmveRi6Th6FJXfHqNHGzsBqtM6clIb63Pdzp0k9+0b6zCUajX0c+hKKRUnNKErpVSc0ISulFJxQhO6UkrFiVb3pmg8OPGtVfux5zJ3HVeZded4naGqpi5gmZN28FjHsz4h1nF/DjdEXN4fI/eOB491AsXqu+0T61l39lTWs+3rioDb9vxkaqB9EarMc9t++yngPvWPNVD7mOBloY6R57FZ/00tdcVfOz5GBDye/tv2bYcAdf33aYBtN7hPA7TnszHPusYYtn1Zw87lO0NuO1i/XHVDnQdefXB4jALF6ls31HlwIq7A++mUmroofPG/FSb051d+yR+WHKPd8vcA145qOFm4yoM/mU6s4+yAhygL0k6T/eftCDXUiny4NNYRNL9P1sQ6guZXvDnWEfjx+PSu/Vh8HuP+Jil+dV2PJWA7lw+IzsWRVpfQ+3XpQG63RHr16o5rF/l8HNu69SgD/4PSUN0THzf2PoiEPFD+7Z1Yz/9kCFbXd9sAO3fs4NTvnhqiX/5xndh0oBPRp8ynPc94G96noU7sAO0Eac+3/ubNmxk6NCvotk+sF/xY++9T7217f1zf+TEiSB/Es50gx8j9uXf8t/3JJ2sZMXyE1z71P54B4nJwjELtpxOxB27Hc9tO9pO73MHzccXy5YwdOzbgsXFvM0Csvtt2sp9O9DNIOxH+XkEw+tV/2/iB3TB7UygszIl1KM2qSEopLDwt1mE0q/RDWynM6RXrMJrV4S8Sye3bKdZhNKu0ZKFzx+RYhxEX9E1RpZSKE5rQlVIqTmhCV0qpOKEJXSml4oQmdKWUihOa0JVSKk5oQldKqTjhKKGLyGQR2SIi20VkdoDyFBFZaJd/LCL9Ix2oUkqp0BpM6CKSCDwBnA9kAVeJSJZPtR8Bh4wxpwGPAg9HOlCllFKhORmhjwK2G2N2GGOOAy8BU3zqTAGete8vAs6R5voOrVJKKcDZV/97A7s9HpcCo4PVMcbUikg50BU44FlJRGYAM+yHlSKyJZygsX4z+UCDteKL9rlt0D63DU3p83eCFTTrXC7GmPnA/Ka2IyJrjDEFEQip1dA+tw3a57YhWn12csllD+D5S7197GUB64hIEpABlEUiQKWUUs44SeirgQEikikiycCVwBs+dd4AfmDfvwz4wPjOpK+UUiqqGrzkYl8TvwV4B0gEnjHGbBKR+4A1xpg3gKeB50VkO3AQK+lHU5Mv27RC2ue2QfvcNkSlz6IDaaWUig/6TVGllIoTmtCVUipOtNiELiLPiMg3IrIxSLmIyGP2dAPrRWR4c8cYaQ76PN3u6wYRWSEiuc0dY6Q11GePeiNFpFZELmuu2KLFSZ9FpFBE1onIJhH5b3PGFw0Ozu0MEfmXiHxm9/mG5o4xkkSkr4gsEZHNdn9+FqBOxHNYi03owAJgcojy84EB9t8M4MlmiCnaFhC6zzuB8caYYcBviY83kxYQus+u6SceBt5tjoCawQJC9FlEOgF/Bi42xgwFLm+muKJpAaGP8/8Am40xuUAh8Af7U3WtVS1wmzEmCzgd+J8AU6ZEPIe12IRujFmK9YmZYKYAzxnLSqCTiPRsnuiio6E+G2NWGGMO2Q9XYn0noFVzcJwBfgq8AnwT/Yiiz0Gfrwb+aYzZZddv9f120GcDpNtThqTZdWubI7ZoMMbsM8Z8Yt+vAIqxvlHvKeI5rMUmdAcCTUngu8Pi2Y+AxbEOItpEpDdwCfHxCsypgUBnESkSkbUicl2sA2oGjwNDgL3ABuBnxpj62IYUGfbss/nAxz5FEc9hzfrVfxUZIjIBK6GfGetYmsEfgVnGmPo2NN9bEjACOAdoD3wkIiuNMVtjG1ZUTQLWAWcD3wX+IyLLjDFHYhtW04hIGtary583R19ac0J3MiVB3BGRHOCvwPnGmLYwvUIB8JKdzE8GLhCRWmPMa7ENK6pKgTJjzFHgqIgsBXKBeE7oNwAP2d8w3y4iO4HBwKrYhhU+EWmHlcxfMMb8M0CViOew1nzJ5Q3gOvud4tOBcmPMvlgHFU0i0g/4J3BtnI/W3IwxmcaY/saY/lhTM/8kzpM5wOvAmSKSJCIdsGY3LY5xTNG2C+sVCSLSAxgE7IhpRE1gvxfwNFBsjHkkSLWI57AWO0IXkb9jvdt9soiUAr8G2gEYY+YBbwEXANuBY1j/4Vs1B32egzUt8Z/tEWtta5+lzkGf405DfTbGFIvI28B6oB74qzEm5Mc6WzoHx/m3wAIR2QAI1mW21jyl7ljgWmCDiKyzl/0K6AfRy2H61X+llIoTrfmSi1JKKQ+a0JVSKk5oQldKqTihCV0ppeKEJnSllIoTmtCVUipOaEJXSqk48f8DVHLttm5a354AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r  4%|         | 2/50 [17:08<6:51:07, 513.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 3 of 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxcJZ3ItE22z",
        "outputId": "e3cbfb9b-5dae-4ef1-d4a6-b7abc243547e"
      },
      "source": [
        "### This cell saves the weights of the trained network for future use.\n",
        "\n",
        "if not LOAD_TRAINED:\n",
        "    torch.save(net.state_dict(), 'siamunet_conc_net_final.pth.tar')\n",
        "    print('SAVE OK')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SAVE OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU_BV2LPE220",
        "outputId": "9c2bff7a-8f51-4b0b-ac53-726df1ac639d"
      },
      "source": [
        "### This cell outputs the results of the trained network when applied to the test set. \n",
        "### Results come in the form of png files showing the network's predictions of change.\n",
        "\n",
        "def save_test_results(dset):\n",
        "    for name in tqdm(dset.names):\n",
        "        with warnings.catch_warnings():\n",
        "            I1, I2, cm = dset.get_img(name)\n",
        "            I1 = Variable(torch.unsqueeze(I1, 0).float()).cuda()\n",
        "            I2 = Variable(torch.unsqueeze(I2, 0).float()).cuda()\n",
        "            out = net(I1, I2)\n",
        "            _, predicted = torch.max(out.data, 1)\n",
        "            I = np.stack((255*cm,255*np.squeeze(predicted.cpu().numpy()),255*cm),2)\n",
        "            io.imsave(f'{net_name}-{name}.png',I)\n",
        "\n",
        "\n",
        "\n",
        "t_start = time.time()\n",
        "# save_test_results(train_dataset)\n",
        "save_test_results(test_dataset)\n",
        "t_end = time.time()\n",
        "print('Elapsed time: {}'.format(t_end - t_start))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: FresUNet-brasilia.png is a low contrast image\n",
            "  if sys.path[0] == '':\n",
            "WARNING:root:Lossy conversion from int64 to uint8. Range [0, 255]. Convert image to uint8 prior to saving to suppress this warning.\n",
            " 10%|         | 1/10 [00:00<00:00,  9.91it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: FresUNet-montpellier.png is a low contrast image\n",
            "  if sys.path[0] == '':\n",
            "WARNING:root:Lossy conversion from int64 to uint8. Range [0, 255]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: FresUNet-norcia.png is a low contrast image\n",
            "  if sys.path[0] == '':\n",
            "WARNING:root:Lossy conversion from int64 to uint8. Range [0, 255]. Convert image to uint8 prior to saving to suppress this warning.\n",
            " 30%|       | 3/10 [00:00<00:00, 11.12it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: FresUNet-rio.png is a low contrast image\n",
            "  if sys.path[0] == '':\n",
            "WARNING:root:Lossy conversion from int64 to uint8. Range [0, 255]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: FresUNet-saclay_w.png is a low contrast image\n",
            "  if sys.path[0] == '':\n",
            "WARNING:root:Lossy conversion from int64 to uint8. Range [0, 255]. Convert image to uint8 prior to saving to suppress this warning.\n",
            " 50%|     | 5/10 [00:00<00:00, 10.45it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: FresUNet-valencia.png is a low contrast image\n",
            "  if sys.path[0] == '':\n",
            "WARNING:root:Lossy conversion from int64 to uint8. Range [0, 255]. Convert image to uint8 prior to saving to suppress this warning.\n",
            " 60%|    | 6/10 [00:00<00:00,  9.88it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: FresUNet-dubai.png is a low contrast image\n",
            "  if sys.path[0] == '':\n",
            "WARNING:root:Lossy conversion from int64 to uint8. Range [0, 255]. Convert image to uint8 prior to saving to suppress this warning.\n",
            " 70%|   | 7/10 [00:00<00:00,  7.49it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: FresUNet-lasvegas.png is a low contrast image\n",
            "  if sys.path[0] == '':\n",
            "WARNING:root:Lossy conversion from int64 to uint8. Range [0, 255]. Convert image to uint8 prior to saving to suppress this warning.\n",
            " 80%|  | 8/10 [00:01<00:00,  6.11it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: FresUNet-milano.png is a low contrast image\n",
            "  if sys.path[0] == '':\n",
            "WARNING:root:Lossy conversion from int64 to uint8. Range [0, 255]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: FresUNet-chongqing.png is a low contrast image\n",
            "  if sys.path[0] == '':\n",
            "WARNING:root:Lossy conversion from int64 to uint8. Range [0, 255]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "100%|| 10/10 [00:01<00:00,  7.85it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Elapsed time: 1.279090404510498\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXF_BPcwE220",
        "outputId": "6d53231d-9b99-47fa-a413-2a91025ca264"
      },
      "source": [
        "### This cell returns various metrics that relate to the performance of the network.\n",
        "### It does this by testing the trained network on the test set (called by test) and then\n",
        "### computing the various metrics e.g. accuracy, precision, recall.  \n",
        "\n",
        "\n",
        "L = 1024\n",
        "\n",
        "def kappa(tp, tn, fp, fn):\n",
        "    N = tp + tn + fp + fn\n",
        "    p0 = (tp + tn) / N\n",
        "    pe = ((tp+fp)*(tp+fn) + (tn+fp)*(tn+fn)) / (N * N)\n",
        "    \n",
        "    return (p0 - pe) / (1 - pe)\n",
        "\n",
        "def test(dset):\n",
        "    net.eval()\n",
        "    tot_loss = 0\n",
        "    tot_count = 0\n",
        "    tot_accurate = 0\n",
        "    \n",
        "    n = 2\n",
        "    class_correct = list(0. for i in range(n))\n",
        "    class_total = list(0. for i in range(n))\n",
        "    class_accuracy = list(0. for i in range(n))\n",
        "    \n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    \n",
        "    for img_index in tqdm(dset.names):\n",
        "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
        "        \n",
        "        s = cm_full.shape\n",
        "        \n",
        "        for ii in range(ceil(s[0]/L)):\n",
        "            for jj in range(ceil(s[1]/L)):\n",
        "                xmin = L*ii\n",
        "                xmax = min(L*(ii+1),s[1])\n",
        "                ymin = L*jj\n",
        "                ymax = min(L*(jj+1),s[1])\n",
        "                I1 = I1_full[:, xmin:xmax, ymin:ymax]\n",
        "                I2 = I2_full[:, xmin:xmax, ymin:ymax]\n",
        "                cm = cm_full[xmin:xmax, ymin:ymax]\n",
        "\n",
        "                I1 = Variable(torch.unsqueeze(I1, 0).float()).cuda()\n",
        "                I2 = Variable(torch.unsqueeze(I2, 0).float()).cuda()\n",
        "                cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm),0).float()).cuda()\n",
        "\n",
        "                output = net(I1, I2)\n",
        "                    \n",
        "                loss = criterion(output, cm.long())\n",
        "                tot_loss += loss.data * np.prod(cm.size())\n",
        "                tot_count += np.prod(cm.size())\n",
        "\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "                c = (predicted.int() == cm.data.int())\n",
        "                for i in range(c.size(1)):\n",
        "                    for j in range(c.size(2)):\n",
        "                        l = int(cm.data[0, i, j])\n",
        "                        class_correct[l] += c[0, i, j]\n",
        "                        class_total[l] += 1\n",
        "                        \n",
        "                pr = (predicted.int() > 0).cpu().numpy()\n",
        "                gt = (cm.data.int() > 0).cpu().numpy()\n",
        "                \n",
        "                tp += np.logical_and(pr, gt).sum()\n",
        "                tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
        "                fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
        "                fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
        "        \n",
        "    net_loss = tot_loss/tot_count        \n",
        "    net_loss = float(net_loss.cpu().numpy())\n",
        "    \n",
        "    net_accuracy = 100 * (tp + tn)/tot_count\n",
        "    \n",
        "    for i in range(n):\n",
        "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
        "        class_accuracy[i] =  float(class_accuracy[i].cpu().numpy())\n",
        "\n",
        "    prec = tp / (tp + fp)\n",
        "    rec = tp / (tp + fn)\n",
        "    dice = 2 * prec * rec / (prec + rec)\n",
        "    prec_nc = tn / (tn + fn)\n",
        "    rec_nc = tn / (tn + fp)\n",
        "    \n",
        "    pr_rec = [prec, rec, dice, prec_nc, rec_nc]\n",
        "    \n",
        "    k = kappa(tp, tn, fp, fn)\n",
        "    \n",
        "    return {'net_loss': net_loss, \n",
        "            'net_accuracy': net_accuracy, \n",
        "            'class_accuracy': class_accuracy, \n",
        "            'precision': prec, \n",
        "            'recall': rec, \n",
        "            'dice': dice, \n",
        "            'kappa': k}\n",
        "\n",
        "results = test(test_dataset)\n",
        "pprint(results)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 10/10 [01:48<00:00, 10.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'class_accuracy': [85.28779602050781, 84.05628967285156],\n",
            " 'dice': 0.3678776430331693,\n",
            " 'kappa': 0.3129762307460547,\n",
            " 'net_accuracy': 85.22480808805321,\n",
            " 'net_loss': 0.4274086654186249,\n",
            " 'precision': 0.23546524656031456,\n",
            " 'recall': 0.8405628747513251}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up-DyoilE221"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLkl8KhjE221"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fa4VoPaE221"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQvzXKopE221"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pxT-us0E222"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}